import pandas as pd
import numpy as np
import optuna
import mlflow
import pickle  # <--- Î‘Ï€Î±ÏÎ±Î¯Ï„Î·Ï„Î¿ Î³Î¹Î± Ï„Î·Î½ Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Ï„Î¿Ï… Scaler
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, f1_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import mlflow_helper

# --- Î¡Î¥Î˜ÎœÎ™Î£Î•Î™Î£ ---
PARQUET_FILE = ""  # <-- Î’Î¬Î»Îµ Ï„Î¿ ÏƒÏ‰ÏƒÏ„ÏŒ path
EMBEDDING_COL = ""  # <-- ÎŒÎ½Î¿Î¼Î± ÏƒÏ„Î®Î»Î·Ï‚ embeddings
TARGET_COL = ""  # <-- ÎŒÎ½Î¿Î¼Î± ÏƒÏ„Î®Î»Î·Ï‚ ÏƒÏ„ÏŒÏ‡Î¿Ï… (0/1)


def load_and_prep_data():
    print("â³ Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Parquet Î±ÏÏ‡ÎµÎ¯Î¿Ï…...")
    try:
        df = pd.read_parquet(PARQUET_FILE)
    except FileNotFoundError:
        print(f"âŒ Î¤Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ {PARQUET_FILE} Î´ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ. Î•Î»ÎµÎ³Î¾Îµ Ï„Î¿ path.")
        exit()
    except Exception as e:
        print(f"âŒ ÎšÎ¬Ï„Î¹ Ï€Î®Î³Îµ ÏƒÏ„ÏÎ±Î²Î¬ Î¼Îµ Ï„Î· Ï†ÏŒÏÏ„Ï‰ÏƒÎ·: {e}")
        exit()

    # ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® Ï„Î·Ï‚ ÏƒÏ„Î®Î»Î·Ï‚ embeddings (Ï€Î¿Ï… ÎµÎ¯Î½Î±Î¹ Î»Î¯ÏƒÏ„Î±) ÏƒÎµ 2D numpy array
    X = np.stack(df[EMBEDDING_COL].values)
    y = df[TARGET_COL].values

    print(f"âœ… Î”ÎµÎ´Î¿Î¼Î­Î½Î± Ï†Î¿ÏÏ„ÏŽÎ¸Î·ÎºÎ±Î½. Shape: {X.shape}")
    return X, y


def get_data_splits(X, y):
    # 1. Test set (15%)
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=0.15, random_state=42, stratify=y
    )

    # 2. Train (70% Î±ÏÏ‡Î¹ÎºÎ¿Ï) ÎºÎ±Î¹ Val (15% Î±ÏÏ‡Î¹ÎºÎ¿Ï)
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=0.1765, random_state=42, stratify=y_temp
    )

    return X_train, X_val, X_test, y_train, y_val, y_test


def objective(trial, X_tr, y_tr, X_v, y_v, normalization_status):
    # --- Î¥Î Î•Î¡-Î Î‘Î¡Î‘ÎœÎ•Î¤Î¡ÎŸÎ™ Î“Î™Î‘ GRADIENT BOOSTING ---
    params = {
        "n_estimators": trial.suggest_int("n_estimators", 50, 300),
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
        "max_depth": trial.suggest_int("max_depth", 3, 10),
        "min_samples_split": trial.suggest_int("min_samples_split", 2, 20),
        "subsample": trial.suggest_float("subsample", 0.6, 1.0),
        # Î¤Î¿ random_state Î²Î¿Î·Î¸Î¬ÎµÎ¹ Î½Î± ÎµÎ¯Î½Î±Î¹ ÏƒÏ„Î±Î¸ÎµÏÎ¬ Ï„Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î±
        "random_state": 42,
        "normalization": normalization_status
    }

    # Î¦Î¹Î»Ï„ÏÎ¬ÏÎ¿Ï…Î¼Îµ Ï„Î¿ 'normalization' Ï€ÏÎ¹Î½ Ï„Î¿ Ï€ÎµÏÎ¬ÏƒÎ¿Ï…Î¼Îµ ÏƒÏ„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿
    model_params = {k: v for k, v in params.items() if k != 'normalization'}

    model = GradientBoostingClassifier(**model_params)
    model.fit(X_tr, y_tr)

    # Î‘Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ·
    preds = model.predict(X_v)
    acc = accuracy_score(y_v, preds)
    f1 = f1_score(y_v, preds)

    metrics = {"val_accuracy": acc, "val_f1": f1}

    # ÎšÎ±Ï„Î±Î³ÏÎ±Ï†Î® Trial
    mlflow_helper.log_optuna_trial(trial, params, metrics, model, "gb_model")

    return f1


def run_experiment_scenario(scenario_name, X_tr, y_tr, X_v, y_v, use_norm, scaler_obj=None):
    print(f"\nðŸš€ ÎˆÎ½Î±ÏÎ¾Î· ÏƒÎµÎ½Î±ÏÎ¯Î¿Ï…: {scenario_name}")

    mlflow_helper.setup_mlflow("Clickbait_GradientBoosting_Comparison")

    with mlflow.start_run(run_name=scenario_name) as run:
        mlflow.log_param("normalization_used", use_norm)

        # Sampler Î³Î¹Î± ÏƒÏ„Î±Î¸ÎµÏÎ¬ Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± (reproducibility)
        sampler = optuna.samplers.TPESampler(seed=42)
        study = optuna.create_study(direction="maximize", sampler=sampler)

        study.optimize(lambda trial: objective(trial, X_tr, y_tr, X_v, y_v, str(use_norm)), n_trials=15)

        print(f"ðŸ† Best params for {scenario_name}: {study.best_params}")

        # --- Î¤Î•Î›Î™ÎšÎ— Î•ÎšÎ Î‘Î™Î”Î•Î¥Î£Î— CHAMPION MODEL ---
        print("âš™ï¸ Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· Ï„Î¿Ï… Champion Model...")
        best_params = study.best_params

        # Î ÏÎ¿ÏƒÎ¸Î­Ï„Î¿Ï…Î¼Îµ Ï„Î¿ random_state ÎºÎ±Î¹ ÎµÎ´ÏŽ Î³Î¹Î± ÏƒÎ¹Î³Î¿Ï…ÏÎ¹Î¬, Î±Î½ Î´ÎµÎ½ Ï„Î¿ Î­Î²Î³Î±Î»Îµ Ï„Î¿ optuna
        if "random_state" not in best_params:
            best_params["random_state"] = 42

        final_model = GradientBoostingClassifier(**best_params)

        # ÎˆÎ½Ï‰ÏƒÎ· Train + Val
        X_full_train = np.concatenate((X_tr, X_v))
        y_full_train = np.concatenate((y_tr, y_v))

        # ... (Î¿ ÎºÏŽÎ´Î¹ÎºÎ±Ï‚ Ï€Î¿Ï… ÎµÎ¯Ï‡ÎµÏ‚ Î³Î¹Î± Ï„Î¿ fit Ï„Î¿Ï… final_model) ...
        final_model.fit(X_full_train, y_full_train)

        # Î•. Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Ï„Î¿Ï… Î¼Î¿Î½Ï„Î­Î»Î¿Ï…
        mlflow.sklearn.log_model(final_model, artifact_path="champion_model")

        # --- ÎÎ•ÎŸ ÎšÎŸÎœÎœÎ‘Î¤Î™: Î Î›Î—Î¡Î—Î£ Î‘ÎžÎ™ÎŸÎ›ÎŸÎ“Î—Î£Î— ---
        # ÎšÎ±Î»Î¿ÏÎ¼Îµ Ï„Î· Î½Î­Î± ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· Î±Ï€ÏŒ Ï„Î¿ helper
        # Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Ï„Î¿ X_test Ï€Î¿Ï… ÎµÎ¯Ï‡Î±Î¼Îµ ÎºÏÎ±Ï„Î®ÏƒÎµÎ¹ ÏƒÏ„Î·Î½ Î¬ÎºÏÎ· ÎºÎ±Î¹ Î´ÎµÎ½ Ï„Î¿ Î±ÎºÎ¿ÏÎ¼Ï€Î·ÏƒÎµ ÎºÎ±Î½ÎµÎ¯Ï‚!
        print("ðŸ“ˆ Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ Ï„ÎµÎ»Î¹ÎºÏŽÎ½ Î¼ÎµÏ„ÏÎ¹ÎºÏŽÎ½ ÏƒÏ„Î¿ Test Set...")

        # Î Î¡ÎŸÎ£ÎŸÎ§Î—: Î‘Î½ Î­Ï‡ÎµÎ¹Ï‚ scaler, Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± Î¼ÎµÏ„Î±Ï„ÏÎ­ÏˆÎµÎ¹Ï‚ Ï„Î¿ Test set!
        if use_norm and scaler_obj is not None:
            # Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Ï„Î¿Î½ scaler Ï€Î¿Ï… Î¼ÏŒÎ»Î¹Ï‚ ÎµÎºÏ€Î±Î¹Î´ÎµÏÏƒÎ±Î¼Îµ/Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÎ±Î¼Îµ
            X_test_final = scaler_obj.transform(X_test)
        else:
            X_test_final = X_test

        # Î•Î´ÏŽ Î³Î¯Î½ÎµÏ„Î±Î¹ Î· ÎºÎ±Ï„Î±Î³ÏÎ±Ï†Î® ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ Î³ÏÎ±Ï†Î·Î¼Î¬Ï„Ï‰Î½ ÎºÎ±Î¹ metrics
        mlflow_helper.evaluate_and_log_metrics(final_model, X_test_final, y_test, prefix="test")

        print(f"âœ… ÎŸÎ»Î¿ÎºÎ»Î·ÏÏŽÎ¸Î·ÎºÎµ. Run ID: {run.info.run_id}")


if __name__ == "__main__":
    if not PARQUET_FILE:
        print("âš ï¸ Î Î¡ÎŸÎ£ÎŸÎ§Î—: Î”ÎµÎ½ Î­Ï‡ÎµÎ¹Ï‚ Î¿ÏÎ¯ÏƒÎµÎ¹ Ï„Î¿ PARQUET_FILE, EMBEDDING_COL Î® TARGET_COL!")
    else:
        # 1. Î¦ÏŒÏÏ„Ï‰ÏƒÎ· ÎºÎ±Î¹ Split
        X, y = load_and_prep_data()
        X_train, X_val, X_test, y_train, y_val, y_test = get_data_splits(X, y)

        # ==========================================
        # Î Î•Î¡Î™Î Î¤Î©Î£Î— 1: Î§Î©Î¡Î™Î£ NORMALIZATION
        # ==========================================
        run_experiment_scenario(
            scenario_name="GB_Raw_Data",
            X_tr=X_train, y_tr=y_train,
            X_v=X_val, y_v=y_val,
            use_norm=False,
            scaler_obj=None  # Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ scaler ÎµÎ´ÏŽ
        )

        # ==========================================
        # Î Î•Î¡Î™Î Î¤Î©Î£Î— 2: ÎœÎ• NORMALIZATION
        # ==========================================
        print("\nâš–ï¸ Î•Ï†Î±ÏÎ¼Î¿Î³Î® Normalization (StandardScaler)...")
        scaler = StandardScaler()

        # Fit Î¼ÏŒÎ½Î¿ ÏƒÏ„Î¿ Train!
        X_train_scaled = scaler.fit_transform(X_train)
        X_val_scaled = scaler.transform(X_val)
        X_test_scaled = scaler.transform(X_test)

        run_experiment_scenario(
            scenario_name="GB_Normalized_Data",
            X_tr=X_train_scaled, y_tr=y_train,
            X_v=X_val_scaled, y_v=y_val,
            use_norm=True,
            scaler_obj=scaler  # Î ÎµÏÎ½Î¬Î¼Îµ Ï„Î¿Î½ scaler Î³Î¹Î± Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ·
        )

        print("\nâœ… ÎŸÎ»Î¿ÎºÎ»Î·ÏÏŽÎ¸Î·ÎºÎ±Î½ ÎºÎ±Î¹ Ï„Î± Î´ÏÎ¿ ÏƒÎµÎ½Î¬ÏÎ¹Î±. ÎˆÎ»ÎµÎ³Î¾Îµ Ï„Î¿ MLflow UI!")