import pandas as pd
import numpy as np
import optuna
import mlflow
import pickle
from sklearn.ensemble import GradientBoostingClassifier
# Î‘Î½ Î¸ÎµÏ‚ Ï€Î¹Î¿ Î³ÏÎ®Î³Î¿ÏÎ¿ training, Î¬Î»Î»Î±Î¾Îµ Ï„Î¿ Ï€Î±ÏÎ±Ï€Î¬Î½Ï‰ ÏƒÎµ:
# from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.metrics import accuracy_score, f1_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import sys
import os

# --- Î¡Î¥Î˜ÎœÎ™Î£Î•Î™Î£ PATHS & MLFLOW HELPER ---
# Î’ÏÎ¯ÏƒÎºÎ¿Ï…Î¼Îµ Ï„Î¿Î½ Ï†Î¬ÎºÎµÎ»Î¿ Ï€Î¿Ï… ÎµÎ¯Î½Î±Î¹ Ï„Î¿ script ÎºÎ±Î¹ Ï€Î¬Î¼Îµ Ï€Î¯ÏƒÏ‰ Î³Î¹Î± Î½Î± Î²ÏÎ¿ÏÎ¼Îµ Ï„Î¿ helper
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
import mlflow_helper

# Î”Ï…Î½Î±Î¼Î¹ÎºÏŒÏ‚ ÎµÎ½Ï„Î¿Ï€Î¹ÏƒÎ¼ÏŒÏ‚ Ï„Ï‰Î½ Î±ÏÏ‡ÎµÎ¯Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½
current_script_path = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(current_script_path, '../../..'))
DATA_FOLDER_NAME = "merged"
data_path = os.path.join(project_root, 'data', DATA_FOLDER_NAME)

PARQUET_FILE = os.path.join(data_path, "data_merged_embed.parquet")
CSV_FILE = os.path.join(data_path, "data_merged.csv")
TARGET_COL = "label"


def load_and_prep_data():
    print("â³ Î¦ÏŒÏÏ„Ï‰ÏƒÎ· ÎºÎ±Î¹ ÏƒÏ…Î³Ï‡ÏÎ½ÎµÏ…ÏƒÎ· Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½...")

    # 1. Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Embeddings
    try:
        df_emb = pd.read_parquet(PARQUET_FILE)
    except FileNotFoundError:
        print(f"âŒ Î¤Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ {PARQUET_FILE} Î´ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ.")
        exit()

    # 2. Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Labels
    try:
        df_lbl = pd.read_csv(CSV_FILE)
    except FileNotFoundError:
        print(f"âŒ Î¤Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ {CSV_FILE} Î´ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ.")
        exit()

    # 3. ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î£Ï…Î¼Î²Î±Ï„ÏŒÏ„Î·Ï„Î±Ï‚
    if len(df_emb) != len(df_lbl):
        print(f"âŒ Î£Ï†Î¬Î»Î¼Î±: Î¤Î± Î±ÏÏ‡ÎµÎ¯Î± Î´ÎµÎ½ Ï„Î±Î¹ÏÎ¹Î¬Î¶Î¿Ï…Î½! Embeddings: {len(df_emb)}, Labels: {len(df_lbl)}")
        exit()

    # 4. Î•Î¾Î±Î³Ï‰Î³Î® Ï„Î¿Ï… X (Embeddings)
    print(f"â„¹ï¸ Î¤Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ Parquet Î­Ï‡ÎµÎ¹ {len(df_emb.columns)} ÏƒÏ„Î®Î»ÎµÏ‚.")

    if len(df_emb.columns) > 1:
        print("â„¹ï¸ Î‘Î½Î¯Ï‡Î½ÎµÏ…ÏƒÎ· Ï€Î¿Î»Î»Î±Ï€Î»ÏÎ½ ÏƒÏ„Î·Î»ÏÎ½. Î§ÏÎ®ÏƒÎ· ÏŒÎ»Î¿Ï… Ï„Î¿Ï… DataFrame Ï‰Ï‚ features.")
        X = df_emb.values  
    else:
        col_name = df_emb.columns[0]
        print(f"â„¹ï¸ Î‘Î½Î¯Ï‡Î½ÎµÏ…ÏƒÎ· Î¼Î¯Î±Ï‚ ÏƒÏ„Î®Î»Î·Ï‚ ('{col_name}'). ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® Î»Î¹ÏƒÏ„ÏÎ½ ÏƒÎµ numpy array.")
        X = np.stack(df_emb[col_name].values)

    # 5. Î•Î¾Î±Î³Ï‰Î³Î® Ï„Î¿Ï… y (Labels)
    y = df_lbl[TARGET_COL].values.astype(int)

    print(f"âœ… Î”ÎµÎ´Î¿Î¼Î­Î½Î± Ï†Î¿ÏÏ„ÏÎ¸Î·ÎºÎ±Î½. X Shape: {X.shape}, y Shape: {y.shape}")
    return X, y


def get_data_splits(X, y):
    # 1. Test set (15%)
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=0.15, random_state=42, stratify=y
    )

    # 2. Train (70% Î±ÏÏ‡Î¹ÎºÎ¿Ï) ÎºÎ±Î¹ Val (15% Î±ÏÏ‡Î¹ÎºÎ¿Ï)
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=0.1765, random_state=42, stratify=y_temp
    )

    return X_train, X_val, X_test, y_train, y_val, y_test


def objective(trial, X_tr, y_tr, X_v, y_v):
    # --- Search Space ---
    params = {
        "n_estimators": trial.suggest_int("n_estimators", 50, 300),
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
        "max_depth": trial.suggest_int("max_depth", 3, 10),
        "min_samples_split": trial.suggest_int("min_samples_split", 2, 20),
        "subsample": trial.suggest_float("subsample", 0.6, 1.0),
        "random_state": 42
    }

    model = GradientBoostingClassifier(**params)
    model.fit(X_tr, y_tr)

    # Î‘Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ·
    preds = model.predict(X_v)
    acc = accuracy_score(y_v, preds)
    f1 = f1_score(y_v, preds)

    metrics = {"val_accuracy": acc, "val_f1": f1}

    # ÎšÎ±Ï„Î±Î³ÏÎ±Ï†Î® Trial
    mlflow_helper.log_optuna_trial(trial, params, metrics, model, "gb_model")

    return f1


def run_experiment_scenario(scenario_name, X_tr, y_tr, X_v, y_v, X_te, y_te):
    print(f"\nğŸš€ ÎˆÎ½Î±ÏÎ¾Î· ÏƒÎµÎ½Î±ÏÎ¯Î¿Ï…: {scenario_name}")

    mlflow_helper.setup_mlflow("Clickbait_GradientBoosting_Comparison")

    with mlflow.start_run(run_name=scenario_name) as run:
        # Î”ÎµÎ½ Î­Ï‡Î¿Ï…Î¼Îµ normalization, Î¬ÏÎ± Ï„Î¿ Î»Î¿Î³ÎºÎ¬ÏÎ¿Ï…Î¼Îµ Ï‰Ï‚ False
        mlflow.log_param("normalization_used", False)

        # Optuna Setup
        sampler = optuna.samplers.TPESampler(seed=42)
        study = optuna.create_study(direction="maximize", sampler=sampler)

        # Î¤ÏÎ­Ï‡Î¿Ï…Î¼Îµ Ï„Î¿ objective (Î´ÎµÎ½ Ï‡ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹ Ï€Î»Î­Î¿Î½ Ï„Î¿ normalization_status argument)
        study.optimize(lambda trial: objective(trial, X_tr, y_tr, X_v, y_v), n_trials=15)

        print(f"ğŸ† Best params for {scenario_name}: {study.best_params}")

        # --- Î¤Î•Î›Î™ÎšÎ— Î•ÎšÎ Î‘Î™Î”Î•Î¥Î£Î— CHAMPION MODEL ---
        print("âš™ï¸ Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· Ï„Î¿Ï… Champion Model...")
        best_params = study.best_params
        if "random_state" not in best_params:
            best_params["random_state"] = 42

        final_model = GradientBoostingClassifier(**best_params)

        # ÎˆÎ½Ï‰ÏƒÎ· Train + Val
        X_full_train = np.concatenate((X_tr, X_v))
        y_full_train = np.concatenate((y_tr, y_v))

        final_model.fit(X_full_train, y_full_train)

        # Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Ï„Î¿Ï… Î¼Î¿Î½Ï„Î­Î»Î¿Ï…
        mlflow.sklearn.log_model(final_model, artifact_path="champion_model")

        # --- Î¤Î•Î›Î™ÎšÎ— Î‘ÎÎ™ÎŸÎ›ÎŸÎ“Î—Î£Î— (TEST SET) ---
        print("ğŸ“ˆ Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ Ï„ÎµÎ»Î¹ÎºÏÎ½ Î¼ÎµÏ„ÏÎ¹ÎºÏÎ½ ÏƒÏ„Î¿ Test Set...")
        
        # Î•Î´Ï Ï„Î¿ X_te ÎµÎ¯Î½Î±Î¹ ÎºÎ±Î¸Î±ÏÏŒ (raw), ÏŒÏ€Ï‰Ï‚ Î±ÎºÏÎ¹Î²ÏÏ‚ Î²Î³Î®ÎºÎµ Î±Ï€ÏŒ Ï„Î¿ split
        mlflow_helper.evaluate_and_log_metrics(final_model, X_te, y_te, prefix="test")
        
        print(f"âœ… ÎŸÎ»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ. Run ID: {run.info.run_id}")


if __name__ == "__main__":
    if not PARQUET_FILE:
        print("âš ï¸ Î Î¡ÎŸÎ£ÎŸÎ§Î—: Î”ÎµÎ½ Î­Ï‡ÎµÎ¹Ï‚ Î¿ÏÎ¯ÏƒÎµÎ¹ Ï„Î± paths ÏƒÏ‰ÏƒÏ„Î¬!")
    else:
        # 1. Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½
        X, y = load_and_prep_data()
        
        # 2. Î”Î¹Î±Ï‡Ï‰ÏÎ¹ÏƒÎ¼ÏŒÏ‚ (Train/Val/Test)
        X_train, X_val, X_test, y_train, y_val, y_test = get_data_splits(X, y)

        # 3. Î•ÎšÎ¤Î•Î›Î•Î£Î— ÎœÎŸÎÎŸ Î¤ÎŸÎ¥ RAW DATA Î£Î•ÎÎ‘Î¡Î™ÎŸÎ¥
        run_experiment_scenario(
            scenario_name="GB_Raw_Data",
            X_tr=X_train, y_tr=y_train,
            X_v=X_val, y_v=y_val,
            X_te=X_test, y_te=y_test  # Î ÎµÏÎ½Î¬Î¼Îµ ÎºÎ±Î¹ Ï„Î¿ Test set Î¼Î­ÏƒÎ±
        )

        print("\nâœ… Î¤Î­Î»Î¿Ï‚ ÎµÎºÏ„Î­Î»ÎµÏƒÎ·Ï‚!")