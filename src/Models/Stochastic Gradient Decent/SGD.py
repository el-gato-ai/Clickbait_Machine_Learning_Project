import pandas as pd
import numpy as np
import optuna
import mlflow
import pickle  # <--- Î§ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹ Î³Î¹Î± Î½Î± ÏƒÏÏƒÎ¿Ï…Î¼Îµ Ï„Î¿Î½ Scaler
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import accuracy_score, f1_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import mlflow_helper

# --- Î¡Î¥Î˜ÎœÎ™Î£Î•Î™Î£ ---
# ÎœÎ·Î½ Î¾ÎµÏ‡Î¬ÏƒÎµÎ¹Ï‚ Î½Î± Ï„Î± ÏƒÏ…Î¼Ï€Î»Î·ÏÏÏƒÎµÎ¹Ï‚!
PARQUET_FILE = "dataset.parquet"
EMBEDDING_COL = "embeddings"
TARGET_COL = "clickbait"


def load_and_prep_data():
    print("â³ Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½...")
    try:
        df = pd.read_parquet(PARQUET_FILE)
    except FileNotFoundError:
        print(f"âŒ Î¤Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ {PARQUET_FILE} Î´ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ.")
        exit()

    X = np.stack(df[EMBEDDING_COL].values)
    y = df[TARGET_COL].values

    print(f"âœ… Loaded: {X.shape}")
    return X, y


def get_data_splits(X, y):
    # Split 1: Test (15%)
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=0.15, random_state=42, stratify=y
    )
    # Split 2: Val (15% of original -> ~17.65% of temp)
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=0.1765, random_state=42, stratify=y_temp
    )
    return X_train, X_val, X_test, y_train, y_val, y_test


def objective(trial, X_tr, y_tr, X_v, y_v, normalization_status):
    # --- Search Space ---
    loss_type = trial.suggest_categorical("loss", ["hinge", "log_loss", "modified_huber", "perceptron"])
    penalty = trial.suggest_categorical("penalty", ["l2", "l1", "elasticnet"])
    alpha = trial.suggest_float("alpha", 1e-6, 1e-1, log=True)

    params = {
        "loss": loss_type,
        "penalty": penalty,
        "alpha": alpha,
        "max_iter": 1000,
        "early_stopping": True,
        "validation_fraction": 0.1,
        "random_state": 42,
        "normalization": normalization_status
    }

    if penalty == "elasticnet":
        params["l1_ratio"] = trial.suggest_float("l1_ratio", 0.0, 1.0)

    # Î¦Î¹Î»Ï„ÏÎ¬ÏÎ¹ÏƒÎ¼Î± Ï€Î±ÏÎ±Î¼Î­Ï„ÏÏ‰Î½ Ï€Î¿Ï… Î´ÎµÎ½ Î±Î½Î®ÎºÎ¿Ï…Î½ ÏƒÏ„Î¿Î½ SGDClassifier
    model_params = {k: v for k, v in params.items() if k != 'normalization'}

    model = SGDClassifier(**model_params)
    model.fit(X_tr, y_tr)

    preds = model.predict(X_v)
    f1 = f1_score(y_v, preds)
    acc = accuracy_score(y_v, preds)

    metrics = {"val_f1": f1, "val_accuracy": acc}

    mlflow_helper.log_optuna_trial(trial, params, metrics, model, "sgd_model")

    return f1


# --- Î”Î™ÎŸÎ¡Î˜Î©Î£Î—: Î ÏÎ¿ÏƒÎ¸Î­ÏƒÎ±Î¼Îµ Ï„Î¿ ÏŒÏÎ¹ÏƒÎ¼Î± scaler_obj=None ---
def run_experiment_scenario(scenario_name, X_tr, y_tr, X_v, y_v, use_norm, scaler_obj=None):
    print(f"\nğŸš€ ÎˆÎ½Î±ÏÎ¾Î· ÏƒÎµÎ½Î±ÏÎ¯Î¿Ï…: {scenario_name}")

    mlflow_helper.setup_mlflow("Clickbait_SGD_Comparison")

    with mlflow.start_run(run_name=scenario_name) as run:
        mlflow.log_param("normalization_used", use_norm)

        # Î ÏÎ¿ÏƒÎ¸Î®ÎºÎ· Seed Î³Î¹Î± Î½Î± ÎµÎ¯Î½Î±Î¹ Ï€Î¬Î½Ï„Î± Î¯Î´Î¹Î± Ï„Î± trials
        sampler = optuna.samplers.TPESampler(seed=42)
        study = optuna.create_study(direction="maximize", sampler=sampler)

        study.optimize(lambda trial: objective(trial, X_tr, y_tr, X_v, y_v, str(use_norm)), n_trials=20)

        print(f"ğŸ† Best params for {scenario_name}: {study.best_params}")

        # --- FINAL TRAINING ---
        print("âš™ï¸ Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· Ï„Î¿Ï… Champion Model...")
        best_params = study.best_params

        # Î ÏÎ¿ÏƒÎ¿Ï‡Î®: Î¤Î¿ SGDClassifier Î­Ï‡ÎµÎ¹ default l1_ratio=0.15.
        # Î‘Î½ Ï„Î¿ Optuna Î´Î¹Î¬Î»ÎµÎ¾Îµ 'elasticnet', Ï„Î¿ l1_ratio Î¸Î± ÎµÎ¯Î½Î±Î¹ ÏƒÏ„Î¿ best_params.
        # Î‘Î½ Î´Î¹Î¬Î»ÎµÎ¾Îµ 'l2', Î´ÎµÎ½ Î¸Î± ÎµÎ¯Î½Î±Î¹, Î¬ÏÎ± Î¸Î± Ï€Î¬ÏÎµÎ¹ Ï„Î¿ default (Ï€Î¿Ï… Î±Î³Î½Î¿ÎµÎ¯Ï„Î±Î¹ ÏƒÏ„Î¿ l2). ÎŸÏ€ÏŒÏ„Îµ ÎµÎ¯Î½Î±Î¹ ÎŸÎš.
        final_model = SGDClassifier(**best_params)

        # ÎˆÎ½Ï‰ÏƒÎ· Train + Val
        X_full_train = np.concatenate((X_tr, X_v))
        y_full_train = np.concatenate((y_tr, y_v))

        # ... (Î¿ ÎºÏÎ´Î¹ÎºÎ±Ï‚ Ï€Î¿Ï… ÎµÎ¯Ï‡ÎµÏ‚ Î³Î¹Î± Ï„Î¿ fit Ï„Î¿Ï… final_model) ...
        final_model.fit(X_full_train, y_full_train)

        # Î•. Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Ï„Î¿Ï… Î¼Î¿Î½Ï„Î­Î»Î¿Ï…
        mlflow.sklearn.log_model(final_model, artifact_path="champion_model")

        # --- ÎÎ•ÎŸ ÎšÎŸÎœÎœÎ‘Î¤Î™: Î Î›Î—Î¡Î—Î£ Î‘ÎÎ™ÎŸÎ›ÎŸÎ“Î—Î£Î— ---
        # ÎšÎ±Î»Î¿ÏÎ¼Îµ Ï„Î· Î½Î­Î± ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· Î±Ï€ÏŒ Ï„Î¿ helper
        # Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Ï„Î¿ X_test Ï€Î¿Ï… ÎµÎ¯Ï‡Î±Î¼Îµ ÎºÏÎ±Ï„Î®ÏƒÎµÎ¹ ÏƒÏ„Î·Î½ Î¬ÎºÏÎ· ÎºÎ±Î¹ Î´ÎµÎ½ Ï„Î¿ Î±ÎºÎ¿ÏÎ¼Ï€Î·ÏƒÎµ ÎºÎ±Î½ÎµÎ¯Ï‚!
        print("ğŸ“ˆ Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ Ï„ÎµÎ»Î¹ÎºÏÎ½ Î¼ÎµÏ„ÏÎ¹ÎºÏÎ½ ÏƒÏ„Î¿ Test Set...")

        # Î Î¡ÎŸÎ£ÎŸÎ§Î—: Î‘Î½ Î­Ï‡ÎµÎ¹Ï‚ scaler, Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± Î¼ÎµÏ„Î±Ï„ÏÎ­ÏˆÎµÎ¹Ï‚ Ï„Î¿ Test set!
        if use_norm and scaler_obj is not None:
            # Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Ï„Î¿Î½ scaler Ï€Î¿Ï… Î¼ÏŒÎ»Î¹Ï‚ ÎµÎºÏ€Î±Î¹Î´ÎµÏÏƒÎ±Î¼Îµ/Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÎ±Î¼Îµ
            X_test_final = scaler_obj.transform(X_test)
        else:
            X_test_final = X_test

        # Î•Î´Ï Î³Î¯Î½ÎµÏ„Î±Î¹ Î· ÎºÎ±Ï„Î±Î³ÏÎ±Ï†Î® ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ Î³ÏÎ±Ï†Î·Î¼Î¬Ï„Ï‰Î½ ÎºÎ±Î¹ metrics
        mlflow_helper.evaluate_and_log_metrics(final_model, X_test_final, y_test, prefix="test")

        print(f"âœ… ÎŸÎ»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ. Run ID: {run.info.run_id}")


if __name__ == "__main__":
    # ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î±Î½ Î¿ÏÎ¯ÏƒÏ„Î·ÎºÎ±Î½ Ï„Î± paths
    if not PARQUET_FILE:
        print("âš ï¸ Î Î¡ÎŸÎ£ÎŸÎ§Î—: Î”ÎµÎ½ Î­Ï‡ÎµÎ¹Ï‚ Î¿ÏÎ¯ÏƒÎµÎ¹ Ï„Î¿ PARQUET_FILE ÏƒÏ„Î·Î½ Î±ÏÏ‡Î® Ï„Î¿Ï… script!")
    else:
        X, y = load_and_prep_data()
        X_train, X_val, X_test, y_train, y_val, y_test = get_data_splits(X, y)

        # Î£ÎµÎ½Î¬ÏÎ¹Î¿ 1: Raw
        run_experiment_scenario(
            "SGD_Raw_Data",
            X_train, y_train, X_val, y_val,
            use_norm=False,
            scaler_obj=None  # Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ scaler ÎµÎ´Ï
        )

        # Î£ÎµÎ½Î¬ÏÎ¹Î¿ 2: Normalized
        print("\nâš–ï¸ Î•Ï†Î±ÏÎ¼Î¿Î³Î® Normalization (StandardScaler)...")
        scaler = StandardScaler()
        # Fit Î¼ÏŒÎ½Î¿ ÏƒÏ„Î¿ Train!
        X_train_scaled = scaler.fit_transform(X_train)
        X_val_scaled = scaler.transform(X_val)
        # (Î¤Î¿ Test Î´ÎµÎ½ Ï„Î¿ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ ÎµÎ´Ï, Î±Î»Î»Î¬ Î¸Î± Î­Ï€ÏÎµÏ€Îµ Î½Î± Î³Î¯Î½ÎµÎ¹ transform Î±Î½ Ï„Î¿ Î¸Î­Î»Î±Î¼Îµ)

        run_experiment_scenario(
            "SGD_Normalized_Data",
            X_train_scaled, y_train, X_val_scaled, y_val,
            use_norm=True,
            scaler_obj=scaler  # <--- Î ÎµÏÎ½Î¬Î¼Îµ Ï„Î¿Î½ scaler Î³Î¹Î± Î½Î± Î±Ï€Î¿Î¸Î·ÎºÎµÏ…Ï„ÎµÎ¯
        )

        print("\nâœ… Î¤Î­Î»Î¿Ï‚!")