import pandas as pd
import numpy as np
import optuna
import warnings
import mlflow
import sys
import os
import time
from sklearn.linear_model import SGDClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, f1_score
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
import mlflow_helper

# Î‘Î³Î½Î¿Î¿ÏÎ¼Îµ Ï„Î± FutureWarnings Î±Ï€ÏŒ Î²Î¹Î²Î»Î¹Î¿Î¸Î®ÎºÎµÏ‚ ÏŒÏ€Ï‰Ï‚ Ï„Î¿ sklearn Î® Ï„Î¿ optuna
warnings.simplefilter(action='ignore', category=FutureWarning)

# Î”Ï…Î½Î±Î¼Î¹ÎºÏŒÏ‚ ÎµÎ½Ï„Î¿Ï€Î¹ÏƒÎ¼ÏŒÏ‚ Ï„Ï‰Î½ Î±ÏÏ‡ÎµÎ¯Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½
current_script_path = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(current_script_path, '../../..'))
DATA_FOLDER = os.path.join(project_root, 'data', 'clean', 'umap')

def load_split_data(data_path):
    files = {
        "Train": "train_umap_500.parquet",
        "Valid": "valid_umap_500.parquet",
        "Test": "test_umap_500.parquet"
    }

    loaded_data = {}
    possible_label_cols = ['labels', 'label', 'target', 'class', 'is_clickbait']

    print(f"â³ ÎˆÎ½Î±ÏÎ¾Î· Ï†ÏŒÏÏ„Ï‰ÏƒÎ·Ï‚ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Î±Ï€ÏŒ: {data_path}")

    for name, filename in files.items():
        file_path = os.path.join(data_path, filename)

        if not os.path.exists(file_path):
            print(f"âŒ Î¤Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ {filename} Î´ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ.")
            sys.exit(1)

        # 1. Î¦ÏŒÏÏ„Ï‰ÏƒÎ· DataFrame (Î”Î¿ÎºÎ¹Î¼Î® fastparquet -> pyarrow)
        try:
            df = pd.read_parquet(file_path, engine='fastparquet')
        except Exception:
            try:
                df = pd.read_parquet(file_path, engine='pyarrow')
            except Exception as e:
                print(f"â›” Î£Ï†Î¬Î»Î¼Î± ÎºÎ±Ï„Î¬ Ï„Î·Î½ Î±Î½Î¬Î³Î½Ï‰ÏƒÎ· Ï„Î¿Ï… {filename}: {e}")
                sys.exit(1)

        # 2. Î•Î½Ï„Î¿Ï€Î¹ÏƒÎ¼ÏŒÏ‚ Features (X)
        feature_cols = [c for c in df.columns if c.startswith("umap_")]
        if not feature_cols:
            feature_cols = [c for c in df.columns if c not in possible_label_cols]

        # 3. Î•Î½Ï„Î¿Ï€Î¹ÏƒÎ¼ÏŒÏ‚ Labels (y)
        label_col = None
        for col in possible_label_cols:
            if col in df.columns:
                label_col = col
                break

        # Î‘Î½ Î´ÎµÎ½ Î²ÏÎµÎ¸ÎµÎ¯ label, ÏˆÎ¬Ï‡Î½Î¿Ï…Î¼Îµ Ï„Î·Î½ Î¼Î¿Î½Î±Î´Î¹ÎºÎ® ÏƒÏ„Î®Î»Î· Ï€Î¿Ï… Î­Î¼ÎµÎ¹Î½Îµ
        if label_col is None:
            remaining = [c for c in df.columns if c not in feature_cols]
            if len(remaining) == 1:
                label_col = remaining[0]
                print(f"   âš ï¸ {name}: Î‘Ï…Ï„ÏŒÎ¼Î±Ï„Î¿Ï‚ ÎµÎ½Ï„Î¿Ï€Î¹ÏƒÎ¼ÏŒÏ‚ label ÏƒÏ„Î®Î»Î·Ï‚: '{label_col}'")

        if label_col:
            # Î’ÎµÎ²Î±Î¹Ï‰Î½ÏŒÎ¼Î±ÏƒÏ„Îµ ÏŒÏ„Î¹ Ï„Î¿ label Î´ÎµÎ½ ÎµÎ¯Î½Î±Î¹ ÏƒÏ„Î± features
            if label_col in feature_cols:
                feature_cols.remove(label_col)

            y = df[label_col].values.astype(int)
        else:
            # Fallback: ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î³Î¹Î± ÎµÎ¾Ï‰Ï„ÎµÏÎ¹ÎºÏŒ Î±ÏÏ‡ÎµÎ¯Î¿ (Ï€.Ï‡. train_labels.csv)
            prefix = filename.split('_')[0]
            ext_label_path = os.path.join(data_path, f"{prefix}_labels.csv")
            if os.path.exists(ext_label_path):
                print(f"   â„¹ï¸ {name}: Î‘Î½Î¬Î³Î½Ï‰ÏƒÎ· labels Î±Ï€ÏŒ ÎµÎ¾Ï‰Ï„ÎµÏÎ¹ÎºÏŒ Î±ÏÏ‡ÎµÎ¯Î¿ ({prefix}_labels.csv)")
                y = pd.read_csv(ext_label_path).iloc[:, 0].values.astype(int)
            else:
                print(f"â›” Î£Ï†Î¬Î»Î¼Î± ÏƒÏ„Î¿ {name}: Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ ÏƒÏ„Î®Î»Î· label Î¿ÏÏ„Îµ ÎµÎ¾Ï‰Ï„ÎµÏÎ¹ÎºÏŒ Î±ÏÏ‡ÎµÎ¯Î¿.")
                sys.exit(1)

        # 4. ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® ÏƒÎµ Numpy Arrays
        X = df[feature_cols].values.astype(np.float32)

        if len(X) != len(y):
            print(f"âŒ Î‘ÏƒÏ…Î¼Ï†Ï‰Î½Î¯Î± Î´Î¹Î±ÏƒÏ„Î¬ÏƒÎµÏ‰Î½ ÏƒÏ„Î¿ {name}: X={len(X)}, y={len(y)}")
            sys.exit(1)

        loaded_data[name] = (X, y)
        print(f"   âœ… {name} loaded: X={X.shape}, y={y.shape}")

    return loaded_data["Train"], loaded_data["Valid"], loaded_data["Test"]

def objective(trial, X_tr, y_tr, X_v, y_v):
    """
    Objective function Î³Î¹Î± Ï„Î¿ Optuna (SGD).
    """
    # --- Search Space SGD ---
    loss_type = trial.suggest_categorical("loss", ["hinge", "log_loss", "modified_huber", "perceptron"])
    penalty = trial.suggest_categorical("penalty", ["l2", "l1", "elasticnet"])
    alpha = trial.suggest_float("alpha", 1e-6, 1e-1, log=True)

    params = {
        "loss": loss_type,
        "penalty": penalty,
        "alpha": alpha,
        "max_iter": 1000,
        "early_stopping": True,
        "n_iter_no_change": 5,
        "random_state": 42
    }

    if penalty == "elasticnet":
        params["l1_ratio"] = trial.suggest_float("l1_ratio", 0.0, 1.0)

    model = SGDClassifier(**params)

    # --- ÎœÎ­Ï„ÏÎ·ÏƒÎ· Î§ÏÏŒÎ½Î¿Ï… Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·Ï‚ (Trial) ---
    start_time = time.time()
    model.fit(X_tr, y_tr)
    training_time = time.time() - start_time
    # -------------------------------------------

    # Validation
    preds = model.predict(X_v)
    f1 = f1_score(y_v, preds)
    acc = accuracy_score(y_v, preds)

    metrics = {"val_f1": f1, "val_accuracy": acc, "training_time_sec": training_time}

    mlflow_helper.log_optuna_trial(
        trial,
        params,
        metrics,
        model,
        run_name_prefix="SGD_Trial"  # <--- ÎÎµÎºÎ¬Î¸Î±ÏÎ¿ ÏŒÎ½Î¿Î¼Î± Î³Î¹Î± ÎºÎ¬Î¸Îµ trial
    )

    return f1

def run_experiment():
    EXPERIMENT_NAME = "Clickbait_SGD_UMAP_Final"

    # 1. Setup MLflow
    mlflow_helper.setup_mlflow(EXPERIMENT_NAME)
    print(f"\nğŸš€ ÎˆÎ½Î±ÏÎ¾Î· Î ÎµÎ¹ÏÎ¬Î¼Î±Ï„Î¿Ï‚: {EXPERIMENT_NAME}")

    # 2. Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½
    (X_train, y_train), (X_val, y_val), (X_test, y_test) = load_split_data(DATA_FOLDER)

    # --- Î£Î—ÎœÎ‘ÎÎ¤Î™ÎšÎŸ: Scaling Î³Î¹Î± SGD ---
    print("âš–ï¸ Î•Ï†Î±ÏÎ¼Î¿Î³Î® StandardScaler (Î±Ï€Î±ÏÎ±Î¯Ï„Î·Ï„Î¿ Î³Î¹Î± SGD)...")
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_val = scaler.transform(X_val)
    X_test = scaler.transform(X_test)
    # ----------------------------------

    # --- Î¦Î‘Î£Î— 1: Hyperparameter Tuning ---
    print("\nğŸ” Î¦Î‘Î£Î— 1: Î‘Î½Î±Î¶Î®Ï„Î·ÏƒÎ· Î’Î­Î»Ï„Î¹ÏƒÏ„Ï‰Î½ Î Î±ÏÎ±Î¼Î­Ï„ÏÏ‰Î½ (Optuna)...")

    # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¿ÏÎ¼Îµ Î­Î½Î± Parent Run Ï€Î¿Ï… Î¸Î± 'ÎºÏÏÎ²ÎµÎ¹' Î¼Î­ÏƒÎ± Ï„Î¿Ï… ÏŒÎ»Î± Ï„Î± trials
    with mlflow.start_run(run_name="ğŸ”_SGD_Hyperparameter_Tuning") as tuning_run:
        mlflow.log_param("dataset", "UMAP_500")
        mlflow.log_param("scaling", "StandardScaler")

        sampler = optuna.samplers.TPESampler(seed=42)
        study = optuna.create_study(direction="maximize", sampler=sampler)

        # Î•ÎºÏ„Î­Î»ÎµÏƒÎ· 20 trials
        study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val), n_trials=20)

        print(f"ğŸ† Best Params found: {study.best_params}")
        print(f"ğŸ† Best Val F1: {study.best_value:.4f}")

    # --- Î¦Î‘Î£Î— 2: Champion Model Training (ÎÎ•Î§Î©Î¡Î™Î£Î¤ÎŸ RUN) ---
    print("\nğŸ‘‘ Î¦Î‘Î£Î— 2: Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· & Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Champion Model...")

    with mlflow.start_run(run_name="ğŸ‘‘_SGD_Champion_Model") as final_run:
        # ÎšÎ±Ï„Î±Î³ÏÎ¬Ï†Î¿Ï…Î¼Îµ Ï„Î¹Ï‚ Ï€Î±ÏÎ±Î¼Î­Ï„ÏÎ¿Ï…Ï‚ Ï„Î¿Ï… Î½Î¹ÎºÎ·Ï„Î®
        best_params = study.best_params
        best_params.update({
            "max_iter": 1000,
            "early_stopping": True,
            "n_iter_no_change": 5,
            "random_state": 42
        })
        mlflow.log_params(best_params)
        mlflow.log_param("model_type", "SGD_Champion")
        mlflow.log_param("dataset", "UMAP_500")

        final_model = SGDClassifier(**best_params)

        # ÎˆÎ½Ï‰ÏƒÎ· Ï„Ï‰Î½ Î®Î´Î· scaled Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½
        X_full_train = np.concatenate((X_train, X_val))
        y_full_train = np.concatenate((y_train, y_val))

        # ÎœÎ­Ï„ÏÎ·ÏƒÎ· Ï‡ÏÏŒÎ½Î¿Ï… Ï„ÎµÎ»Î¹ÎºÎ®Ï‚ ÎµÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·Ï‚
        start_t = time.time()
        final_model.fit(X_full_train, y_full_train)
        final_train_time = time.time() - start_t
        print(f"â±ï¸ Training Time: {final_train_time:.2f} sec")

        # Log Ï„Î¿Ï… Î¼Î¿Î½Ï„Î­Î»Î¿Ï…
        mlflow.sklearn.log_model(final_model, artifact_path="champion_model")

        # Evaluation ÏƒÏ„Î¿ Test set (Î¼Îµ ÏŒÎ»Î± Ï„Î± advanced metrics)
        print("ğŸ“ˆ Î‘Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ· ÏƒÏ„Î¿ Test Set...")
        mlflow_helper.evaluate_and_log_metrics(
            final_model,
            X_test,
            y_test,
            prefix="test",
            training_time=final_train_time
        )

        print(f"\nâœ… Î¤Î•Î›ÎŸÎ£! Î¤Î¿ Champion Model Î±Ï€Î¿Î¸Î·ÎºÎµÏÏ„Î·ÎºÎµ ÏƒÏ„Î¿ Run ID: {final_run.info.run_id}")
        print(f"   ğŸ‘‰ Î‘Î½Î±Î¶Î·Ï„Î®ÏƒÏ„Îµ ÏƒÏ„Î¿ MLflow UI Ï„Î¿ Run Î¼Îµ ÏŒÎ½Î¿Î¼Î±: 'ğŸ‘‘_SGD_Champion_Model'")

if __name__ == "__main__":
    run_experiment()