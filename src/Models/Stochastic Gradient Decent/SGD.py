import pandas as pd
import numpy as np
import optuna
import mlflow
import pickle
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import accuracy_score, f1_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import sys
import os

# --- Î¡Î¥Î˜ÎœÎ™Î£Î•Î™Î£ PATHS & MLFLOW HELPER ---
# Î’ÏÎ¯ÏƒÎºÎ¿Ï…Î¼Îµ Ï„Î¿Î½ Ï†Î¬ÎºÎµÎ»Î¿ Ï€Î¿Ï… ÎµÎ¯Î½Î±Î¹ Ï„Î¿ script ÎºÎ±Î¹ Ï€Î¬Î¼Îµ Ï€Î¯ÏƒÏ‰ Î³Î¹Î± Î½Î± Î²ÏÎ¿ÏÎ¼Îµ Ï„Î¿ helper
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
import mlflow_helper

# Î”Ï…Î½Î±Î¼Î¹ÎºÏŒÏ‚ ÎµÎ½Ï„Î¿Ï€Î¹ÏƒÎ¼ÏŒÏ‚ Ï„Ï‰Î½ Î±ÏÏ‡ÎµÎ¯Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½
current_script_path = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(current_script_path, '../../..'))
DATA_FOLDER_NAME = "merged"
data_path = os.path.join(project_root, 'data', DATA_FOLDER_NAME)

PARQUET_FILE = os.path.join(data_path, "data_merged_embed.parquet")
CSV_FILE = os.path.join(data_path, "data_merged.csv")
TARGET_COL = "label"


def load_and_prep_data():
    print("â³ Î¦ÏŒÏÏ„Ï‰ÏƒÎ· ÎºÎ±Î¹ ÏƒÏ…Î³Ï‡ÏÎ½ÎµÏ…ÏƒÎ· Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½...")

    # 1. Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Embeddings
    try:
        df_emb = pd.read_parquet(PARQUET_FILE)
    except FileNotFoundError:
        print(f"âŒ Î¤Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ {PARQUET_FILE} Î´ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ.")
        exit()

    # 2. Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Labels
    try:
        df_lbl = pd.read_csv(CSV_FILE)
    except FileNotFoundError:
        print(f"âŒ Î¤Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ {CSV_FILE} Î´ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ.")
        exit()

    # 3. ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î£Ï…Î¼Î²Î±Ï„ÏŒÏ„Î·Ï„Î±Ï‚
    if len(df_emb) != len(df_lbl):
        print(f"âŒ Î£Ï†Î¬Î»Î¼Î±: Î¤Î± Î±ÏÏ‡ÎµÎ¯Î± Î´ÎµÎ½ Ï„Î±Î¹ÏÎ¹Î¬Î¶Î¿Ï…Î½! Embeddings: {len(df_emb)}, Labels: {len(df_lbl)}")
        exit()

    # 4. Î•Î¾Î±Î³Ï‰Î³Î® Ï„Î¿Ï… X (Embeddings)
    print(f"â„¹ï¸ Î¤Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ Parquet Î­Ï‡ÎµÎ¹ {len(df_emb.columns)} ÏƒÏ„Î®Î»ÎµÏ‚.")

    if len(df_emb.columns) > 1:
        print("â„¹ï¸ Î‘Î½Î¯Ï‡Î½ÎµÏ…ÏƒÎ· Ï€Î¿Î»Î»Î±Ï€Î»ÏÎ½ ÏƒÏ„Î·Î»ÏÎ½. Î§ÏÎ®ÏƒÎ· ÏŒÎ»Î¿Ï… Ï„Î¿Ï… DataFrame Ï‰Ï‚ features.")
        X = df_emb.values  
    else:
        col_name = df_emb.columns[0]
        print(f"â„¹ï¸ Î‘Î½Î¯Ï‡Î½ÎµÏ…ÏƒÎ· Î¼Î¯Î±Ï‚ ÏƒÏ„Î®Î»Î·Ï‚ ('{col_name}'). ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® Î»Î¹ÏƒÏ„ÏÎ½ ÏƒÎµ numpy array.")
        X = np.stack(df_emb[col_name].values)

    # 5. Î•Î¾Î±Î³Ï‰Î³Î® Ï„Î¿Ï… y (Labels)
    y = df_lbl[TARGET_COL].values.astype(int)

    print(f"âœ… Î”ÎµÎ´Î¿Î¼Î­Î½Î± Ï†Î¿ÏÏ„ÏÎ¸Î·ÎºÎ±Î½. X Shape: {X.shape}, y Shape: {y.shape}")
    return X, y


def get_data_splits(X, y):
    # 1. Test set (15%)
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=0.15, random_state=42, stratify=y
    )
    # 2. Train (70% Î±ÏÏ‡Î¹ÎºÎ¿Ï) ÎºÎ±Î¹ Val (15% Î±ÏÏ‡Î¹ÎºÎ¿Ï)
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=0.1765, random_state=42, stratify=y_temp
    )
    return X_train, X_val, X_test, y_train, y_val, y_test


def objective(trial, X_tr, y_tr, X_v, y_v, normalization_status):
    # --- Search Space SGD ---
    loss_type = trial.suggest_categorical("loss", ["hinge", "log_loss", "modified_huber", "perceptron"])
    penalty = trial.suggest_categorical("penalty", ["l2", "l1", "elasticnet"])
    alpha = trial.suggest_float("alpha", 1e-6, 1e-1, log=True) # Learning Rate / Regularization strength

    params = {
        "loss": loss_type,
        "penalty": penalty,
        "alpha": alpha,
        "max_iter": 1000,
        "early_stopping": True,
        "validation_fraction": 0.1,
        "n_iter_no_change": 5, # Î‘Î½ Î´ÎµÎ½ Î²ÎµÎ»Ï„Î¹Ï‰Î¸ÎµÎ¯ Î³Î¹Î± 5 Ï†Î¿ÏÎ­Ï‚, ÏƒÏ„Î±Î¼Î¬Ï„Î±
        "random_state": 42,
        "normalization": normalization_status
    }

    if penalty == "elasticnet":
        params["l1_ratio"] = trial.suggest_float("l1_ratio", 0.0, 1.0)

    # Î¦Î¹Î»Ï„ÏÎ¬ÏÎ¹ÏƒÎ¼Î± Ï€Î±ÏÎ±Î¼Î­Ï„ÏÏ‰Î½ Ï€Î¿Ï… Î´ÎµÎ½ Î±Î½Î®ÎºÎ¿Ï…Î½ ÏƒÏ„Î¿Î½ SGDClassifier (ÏŒÏ€Ï‰Ï‚ Ï„Î¿ normalization)
    model_params = {k: v for k, v in params.items() if k != 'normalization'}

    model = SGDClassifier(**model_params)
    model.fit(X_tr, y_tr)

    preds = model.predict(X_v)
    f1 = f1_score(y_v, preds)
    acc = accuracy_score(y_v, preds)

    metrics = {"val_f1": f1, "val_accuracy": acc}

    mlflow_helper.log_optuna_trial(trial, params, metrics, model, "sgd_model")

    return f1


def run_experiment_scenario(scenario_name, X_tr, y_tr, X_v, y_v, X_te, y_te, use_norm, scaler_obj=None):
    print(f"\nğŸš€ ÎˆÎ½Î±ÏÎ¾Î· ÏƒÎµÎ½Î±ÏÎ¯Î¿Ï…: {scenario_name}")

    mlflow_helper.setup_mlflow("Clickbait_SGD_Comparison")

    with mlflow.start_run(run_name=scenario_name) as run:
        mlflow.log_param("normalization_used", use_norm)

        # Sampler
        sampler = optuna.samplers.TPESampler(seed=42)
        study = optuna.create_study(direction="maximize", sampler=sampler)

        study.optimize(lambda trial: objective(trial, X_tr, y_tr, X_v, y_v, str(use_norm)), n_trials=20)

        print(f"ğŸ† Best params for {scenario_name}: {study.best_params}")

        # --- FINAL TRAINING ---
        print("âš™ï¸ Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· Ï„Î¿Ï… Champion Model...")
        best_params = study.best_params
        
        # Î ÏÎ¿ÏƒÎ¸Î®ÎºÎ· defaults Î±Î½ Î»ÎµÎ¯Ï€Î¿Ï…Î½ (Ï€.Ï‡. random_state)
        if "random_state" not in best_params:
            best_params["random_state"] = 42

        # Î¤Î¿ SGDClassifier Ï‡ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹ Ï€ÏÎ¿ÏƒÎ¿Ï‡Î® Î¼Îµ Ï„Î¿ elasticnet ratio
        final_model = SGDClassifier(**best_params)

        # ÎˆÎ½Ï‰ÏƒÎ· Train + Val
        X_full_train = np.concatenate((X_tr, X_v))
        y_full_train = np.concatenate((y_tr, y_v))

        final_model.fit(X_full_train, y_full_train)

        # Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Î¼Î¿Î½Ï„Î­Î»Î¿Ï…
        mlflow.sklearn.log_model(final_model, artifact_path="champion_model")

        # --- EVALUATION ON TEST SET ---
        print("ğŸ“ˆ Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ Ï„ÎµÎ»Î¹ÎºÏÎ½ Î¼ÎµÏ„ÏÎ¹ÎºÏÎ½ ÏƒÏ„Î¿ Test Set...")

        # Î£Î·Î¼Î±Î½Ï„Î¹ÎºÏŒ: Î‘Î½ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ scaler, Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± Î¼ÎµÏ„Î±Ï„ÏÎ­ÏˆÎ¿Ï…Î¼Îµ Ï„Î¿ Test set!
        if use_norm and scaler_obj is not None:
            X_test_final = scaler_obj.transform(X_te)
        else:
            X_test_final = X_te

        mlflow_helper.evaluate_and_log_metrics(final_model, X_test_final, y_te, prefix="test")

        print(f"âœ… ÎŸÎ»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ. Run ID: {run.info.run_id}")


if __name__ == "__main__":
    if not PARQUET_FILE:
        print("âš ï¸ Î Î¡ÎŸÎ£ÎŸÎ§Î—: Î”ÎµÎ½ Î­Ï‡ÎµÎ¹Ï‚ Î¿ÏÎ¯ÏƒÎµÎ¹ Ï„Î± paths ÏƒÏ‰ÏƒÏ„Î¬!")
    else:
        # 1. Î¦ÏŒÏÏ„Ï‰ÏƒÎ·
        X, y = load_and_prep_data()
        X_train, X_val, X_test, y_train, y_val, y_test = get_data_splits(X, y)

        # ==========================================
        # SCENARIO 1: RAW DATA (Î“Î¹Î± ÏƒÏÎ³ÎºÏÎ¹ÏƒÎ· - Î±Î½Î±Î¼Î­Î½Î¿Ï…Î¼Îµ Î½Î± Ï€Î¬ÎµÎ¹ Ï‡ÎµÎ¹ÏÏŒÏ„ÎµÏÎ±)
        # ==========================================
        run_experiment_scenario(
            "SGD_Raw_Data",
            X_train, y_train, X_val, y_val, X_test, y_test,
            use_norm=False,
            scaler_obj=None
        )

        # ==========================================
        # SCENARIO 2: NORMALIZED DATA (Î¤Î¿ Î²Î±ÏƒÎ¹ÎºÏŒ)
        # ==========================================
        print("\nâš–ï¸ Î•Ï†Î±ÏÎ¼Î¿Î³Î® Normalization (StandardScaler)...")
        scaler = StandardScaler()
        
        # Fit Î¼ÏŒÎ½Î¿ ÏƒÏ„Î¿ Train!
        X_train_scaled = scaler.fit_transform(X_train)
        X_val_scaled = scaler.transform(X_val)
        # Î¤Î¿ Test scaled Î³Î¯Î½ÎµÏ„Î±Î¹ transform Î±Ï…Ï„ÏŒÎ¼Î±Ï„Î± Î¼Î­ÏƒÎ± ÏƒÏ„Î· ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· run_experiment_scenario
        
        run_experiment_scenario(
            "SGD_Normalized_Data",
            X_train_scaled, y_train, X_val_scaled, y_val, X_test, y_test,
            use_norm=True,
            scaler_obj=scaler
        )

        print("\nâœ… Î¤Î­Î»Î¿Ï‚!")