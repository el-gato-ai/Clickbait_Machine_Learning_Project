import pandas as pd
import numpy as np
import optuna
import mlflow
import sys
import os
import time
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import accuracy_score, f1_score

# --- Î¡Î¥Î˜ÎœÎ™Î£Î•Î™Î£ PATHS ---
# Î’ÏÎ¯ÏƒÎºÎ¿Ï…Î¼Îµ Ï„Î¿Î½ Ï†Î¬ÎºÎµÎ»Î¿ Ï€Î¿Ï… ÎµÎ¯Î½Î±Î¹ Ï„Î¿ script ÎºÎ±Î¹ Ï€Î¬Î¼Îµ Ï€Î¯ÏƒÏ‰ Î³Î¹Î± Î½Î± Î²ÏÎ¿ÏÎ¼Îµ Ï„Î¿ helper
# (Î¥Ï€Î¿Î¸Î­Ï„Î¿Ï…Î¼Îµ ÏŒÏ„Î¹ Ï„Î¿ script Ï„ÏÎ­Ï‡ÎµÎ¹ Î±Ï€ÏŒ Ï„Î¿Î½ Ï†Î¬ÎºÎµÎ»Î¿ 'Models/Stochastic Gradient Decent')
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
import mlflow_helper

# Î”Ï…Î½Î±Î¼Î¹ÎºÏŒÏ‚ ÎµÎ½Ï„Î¿Ï€Î¹ÏƒÎ¼ÏŒÏ‚ Ï„Ï‰Î½ Î±ÏÏ‡ÎµÎ¯Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½
current_script_path = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(current_script_path, '../../..'))
DATA_FOLDER = os.path.join(project_root, 'data', 'clean', 'umap')


def load_split_data(data_path):
    """
    Î¦Î¿ÏÏ„ÏÎ½ÎµÎ¹ Ï„Î± Î­Ï„Î¿Î¹Î¼Î± Train/Valid/Test Î±ÏÏ‡ÎµÎ¯Î± Parquet.
    - Î”Î¹Î±Ï‡ÎµÎ¹ÏÎ¯Î¶ÎµÏ„Î±Î¹ Î±Ï…Ï„ÏŒÎ¼Î±Ï„Î± fastparquet/pyarrow.
    - Î‘Î½Î±Î³Î½Ï‰ÏÎ¯Î¶ÎµÎ¹ Ï„Î± features ÎºÎ±Î¹ Ï„Î¿ label.
    - Î”ÎµÎ½ ÎµÏ†Î±ÏÎ¼ÏŒÎ¶ÎµÎ¹ scaling (Î²Î¬ÏƒÎµÎ¹ Î¿Î´Î·Î³Î¯Î±Ï‚).
    """
    files = {
        "Train": "train_umap_500.parquet",
        "Valid": "valid_umap_500.parquet",
        "Test": "test_umap_500.parquet"
    }

    loaded_data = {}
    possible_label_cols = ['labels', 'label', 'target', 'class', 'is_clickbait']

    print(f"â³ ÎˆÎ½Î±ÏÎ¾Î· Ï†ÏŒÏÏ„Ï‰ÏƒÎ·Ï‚ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Î±Ï€ÏŒ: {data_path}")

    for name, filename in files.items():
        file_path = os.path.join(data_path, filename)

        if not os.path.exists(file_path):
            print(f"âŒ Î¤Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ {filename} Î´ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ.")
            sys.exit(1)

        # 1. Î¦ÏŒÏÏ„Ï‰ÏƒÎ· DataFrame (Î”Î¿ÎºÎ¹Î¼Î® fastparquet -> pyarrow)
        try:
            df = pd.read_parquet(file_path, engine='fastparquet')
        except Exception:
            try:
                df = pd.read_parquet(file_path, engine='pyarrow')
            except Exception as e:
                print(f"â›” Î£Ï†Î¬Î»Î¼Î± ÎºÎ±Ï„Î¬ Ï„Î·Î½ Î±Î½Î¬Î³Î½Ï‰ÏƒÎ· Ï„Î¿Ï… {filename}: {e}")
                sys.exit(1)

        # 2. Î•Î½Ï„Î¿Ï€Î¹ÏƒÎ¼ÏŒÏ‚ Features (X)
        feature_cols = [c for c in df.columns if c.startswith("umap_")]
        if not feature_cols:
            feature_cols = [c for c in df.columns if c not in possible_label_cols]

        # 3. Î•Î½Ï„Î¿Ï€Î¹ÏƒÎ¼ÏŒÏ‚ Labels (y)
        label_col = None
        for col in possible_label_cols:
            if col in df.columns:
                label_col = col
                break

        # Fallback: Î‘Î½ Î´ÎµÎ½ Î²ÏÎµÎ¸ÎµÎ¯ label, ÏˆÎ¬Ï‡Î½Î¿Ï…Î¼Îµ Î³Î¹Î± ÎµÎ¾Ï‰Ï„ÎµÏÎ¹ÎºÏŒ Î±ÏÏ‡ÎµÎ¯Î¿ Î® Ï„Î·Î½ ÎµÎ½Î±Ï€Î¿Î¼ÎµÎ¯Î½Î±ÏƒÎ± ÏƒÏ„Î®Î»Î·
        if label_col is None:
            remaining = [c for c in df.columns if c not in feature_cols]
            if len(remaining) == 1:
                label_col = remaining[0]
                print(f"   âš ï¸ {name}: Î‘Ï…Ï„ÏŒÎ¼Î±Ï„Î¿Ï‚ ÎµÎ½Ï„Î¿Ï€Î¹ÏƒÎ¼ÏŒÏ‚ label ÏƒÏ„Î®Î»Î·Ï‚: '{label_col}'")
            else:
                # ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î³Î¹Î± ÎµÎ¾Ï‰Ï„ÎµÏÎ¹ÎºÏŒ Î±ÏÏ‡ÎµÎ¯Î¿ labels
                prefix = filename.split('_')[0]
                ext_label_path = os.path.join(data_path, f"{prefix}_labels.csv")
                if os.path.exists(ext_label_path):
                    print(f"   â„¹ï¸ {name}: Î‘Î½Î¬Î³Î½Ï‰ÏƒÎ· labels Î±Ï€ÏŒ ÎµÎ¾Ï‰Ï„ÎµÏÎ¹ÎºÏŒ Î±ÏÏ‡ÎµÎ¯Î¿ ({prefix}_labels.csv)")
                    df_labels = pd.read_csv(ext_label_path)
                    y = df_labels.iloc[:, 0].values.astype(int)
                    # Î•Î´Ï Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± Î¿ÏÎ¯ÏƒÎ¿Ï…Î¼Îµ Ï„Î¿ X ÎºÎ±Î¹ Î½Î± ÏƒÏ…Î½ÎµÏ‡Î¯ÏƒÎ¿Ï…Î¼Îµ
                    X = df[feature_cols].values.astype(np.float32)
                    loaded_data[name] = (X, y)
                    print(f"   âœ… {name} loaded: X={X.shape}, y={y.shape}")
                    continue
                else:
                    print(f"â›” Î£Ï†Î¬Î»Î¼Î± ÏƒÏ„Î¿ {name}: Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ ÏƒÏ„Î®Î»Î· label.")
                    sys.exit(1)

        if label_col:
            if label_col in feature_cols:
                feature_cols.remove(label_col)
            y = df[label_col].values.astype(int)

        # 4. ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® ÏƒÎµ Numpy Arrays
        X = df[feature_cols].values.astype(np.float32)

        if len(X) != len(y):
            print(f"âŒ Î‘ÏƒÏ…Î¼Ï†Ï‰Î½Î¯Î± Î´Î¹Î±ÏƒÏ„Î¬ÏƒÎµÏ‰Î½ ÏƒÏ„Î¿ {name}: X={len(X)}, y={len(y)}")
            sys.exit(1)

        loaded_data[name] = (X, y)
        print(f"   âœ… {name} loaded: X={X.shape}, y={y.shape}")

    return loaded_data["Train"], loaded_data["Valid"], loaded_data["Test"]


def objective(trial, X_tr, y_tr, X_v, y_v):
    """
    Objective function Î³Î¹Î± Ï„Î¿ Optuna (SGD).
    """
    # --- Search Space SGD ---
    loss_type = trial.suggest_categorical("loss", ["hinge", "log_loss", "modified_huber", "perceptron"])
    penalty = trial.suggest_categorical("penalty", ["l2", "l1", "elasticnet"])
    alpha = trial.suggest_float("alpha", 1e-6, 1e-1, log=True)

    params = {
        "loss": loss_type,
        "penalty": penalty,
        "alpha": alpha,
        "max_iter": 1000,
        "early_stopping": True,
        "n_iter_no_change": 5,
        "random_state": 42
    }

    if penalty == "elasticnet":
        params["l1_ratio"] = trial.suggest_float("l1_ratio", 0.0, 1.0)

    model = SGDClassifier(**params)

    # --- ÎœÎ­Ï„ÏÎ·ÏƒÎ· Î§ÏÏŒÎ½Î¿Ï… Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·Ï‚ (Trial) ---
    start_time = time.time()
    model.fit(X_tr, y_tr)
    training_time = time.time() - start_time
    # -------------------------------------------

    # Validation
    preds = model.predict(X_v)
    f1 = f1_score(y_v, preds)
    acc = accuracy_score(y_v, preds)

    metrics = {"val_f1": f1, "val_accuracy": acc, "training_time_sec": training_time}

    # Log ÏƒÏ„Î¿ MLflow Î¼Îµ ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½Î¿ prefix Î¿Î½ÏŒÎ¼Î±Ï„Î¿Ï‚
    mlflow_helper.log_optuna_trial(
        trial,
        params,
        metrics,
        model,
        run_name_prefix="SGD_Trial"
    )

    return f1


def run_experiment():
    EXPERIMENT_NAME = "Clickbait_SGD_UMAP_Final_NoScaling"

    # 1. Setup MLflow
    mlflow_helper.setup_mlflow(EXPERIMENT_NAME)
    print(f"\nğŸš€ ÎˆÎ½Î±ÏÎ¾Î· Î ÎµÎ¹ÏÎ¬Î¼Î±Ï„Î¿Ï‚: {EXPERIMENT_NAME}")

    # 2. Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½
    (X_train, y_train), (X_val, y_val), (X_test, y_test) = load_split_data(DATA_FOLDER)

    print("â„¹ï¸ Î£Î·Î¼ÎµÎ¯Ï‰ÏƒÎ·: Î”ÎµÎ½ ÎµÏ†Î±ÏÎ¼ÏŒÎ¶ÎµÏ„Î±Î¹ Feature Scaling (StandardScaler) ÏƒÏ„Î± UMAP embeddings.")

    # --- Î¦Î‘Î£Î— 1: Hyperparameter Tuning ---
    print("\nğŸ” Î¦Î‘Î£Î— 1: Î‘Î½Î±Î¶Î®Ï„Î·ÏƒÎ· Î’Î­Î»Ï„Î¹ÏƒÏ„Ï‰Î½ Î Î±ÏÎ±Î¼Î­Ï„ÏÏ‰Î½ (Optuna)...")

    # Parent Run Î³Î¹Î± Ï„Î¿ Tuning
    with mlflow.start_run(run_name="ğŸ”_SGD_Hyperparameter_Tuning") as tuning_run:
        mlflow.log_param("dataset", "UMAP_500")
        mlflow.log_param("scaling", "None")

        sampler = optuna.samplers.TPESampler(seed=42)
        study = optuna.create_study(direction="maximize", sampler=sampler)

        # Î•ÎºÏ„Î­Î»ÎµÏƒÎ· 20 trials
        study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val), n_trials=20)

        print(f"ğŸ† Best Params found: {study.best_params}")
        print(f"ğŸ† Best Val F1: {study.best_value:.4f}")

    # --- Î¦Î‘Î£Î— 2: Champion Model Training (ÎÎ•Î§Î©Î¡Î™Î£Î¤ÎŸ RUN) ---
    print("\nğŸ‘‘ Î¦Î‘Î£Î— 2: Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· & Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Champion Model...")

    # ÎÎµÏ‡Ï‰ÏÎ¹ÏƒÏ„ÏŒ Run Î³Î¹Î± Ï„Î¿ Ï„ÎµÎ»Î¹ÎºÏŒ Î¼Î¿Î½Ï„Î­Î»Î¿
    with mlflow.start_run(run_name="ğŸ‘‘_SGD_Champion_Model") as final_run:
        # ÎšÎ±Ï„Î±Î³ÏÎ¬Ï†Î¿Ï…Î¼Îµ Ï„Î¹Ï‚ Ï€Î±ÏÎ±Î¼Î­Ï„ÏÎ¿Ï…Ï‚ Ï„Î¿Ï… Î½Î¹ÎºÎ·Ï„Î®
        best_params = study.best_params
        best_params.update({
            "max_iter": 1000,
            "early_stopping": True,
            "n_iter_no_change": 5,
            "random_state": 42
        })
        mlflow.log_params(best_params)
        mlflow.log_param("model_type", "SGD_Champion")
        mlflow.log_param("dataset", "UMAP_500")

        final_model = SGDClassifier(**best_params)

        # ÎˆÎ½Ï‰ÏƒÎ· Ï„Ï‰Î½ Train + Valid (Ï‡Ï‰ÏÎ¯Ï‚ scaling)
        X_full_train = np.concatenate((X_train, X_val))
        y_full_train = np.concatenate((y_train, y_val))

        # ÎœÎ­Ï„ÏÎ·ÏƒÎ· Ï‡ÏÏŒÎ½Î¿Ï… Ï„ÎµÎ»Î¹ÎºÎ®Ï‚ ÎµÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·Ï‚
        start_t = time.time()
        final_model.fit(X_full_train, y_full_train)
        final_train_time = time.time() - start_t
        print(f"â±ï¸ Training Time: {final_train_time:.2f} sec")

        # Log Ï„Î¿Ï… Î¼Î¿Î½Ï„Î­Î»Î¿Ï…
        mlflow.sklearn.log_model(final_model, artifact_path="champion_model")

        # Evaluation ÏƒÏ„Î¿ Test set
        print("ğŸ“ˆ Î‘Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ· ÏƒÏ„Î¿ Test Set...")
        mlflow_helper.evaluate_and_log_metrics(
            final_model,
            X_test,
            y_test,
            prefix="test",
            training_time=final_train_time
        )

        print(f"\nâœ… Î¤Î•Î›ÎŸÎ£! Î¤Î¿ Champion Model Î±Ï€Î¿Î¸Î·ÎºÎµÏÏ„Î·ÎºÎµ ÏƒÏ„Î¿ Run ID: {final_run.info.run_id}")
        print(f"   ğŸ‘‰ Î‘Î½Î±Î¶Î·Ï„Î®ÏƒÏ„Îµ ÏƒÏ„Î¿ MLflow UI Ï„Î¿ Run Î¼Îµ ÏŒÎ½Î¿Î¼Î±: 'ğŸ‘‘_SGD_Champion_Model'")


if __name__ == "__main__":
    run_experiment()