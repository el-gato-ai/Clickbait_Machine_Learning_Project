import mlflow
import matplotlib.pyplot as plt
import seaborn as sns
import time
from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score,
                             roc_auc_score, confusion_matrix, roc_curve)


def evaluate_and_log_metrics(model, X_test, y_test, prefix="test"):
    """
    Î¥Ï€Î¿Î»Î¿Î³Î¯Î¶ÎµÎ¹ metrics, Ï†Ï„Î¹Î¬Ï‡Î½ÎµÎ¹ Î³ÏÎ±Ï†Î®Î¼Î±Ï„Î± ÎºÎ±Î¹ Ï„Î± ÏƒÏ„Î­Î»Î½ÎµÎ¹ ÏƒÏ„Î¿ MLflow.
    prefix: 'val' Î³Î¹Î± validation set, 'test' Î³Î¹Î± test set.
    """
    start_time = time.time()

    # 1. Î ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚
    predictions = model.predict(X_test)

    # Î ÏÎ¿ÏƒÏ€Î¬Î¸ÎµÎ¹Î± Î»Î®ÏˆÎ·Ï‚ Ï€Î¹Î¸Î±Î½Î¿Ï„Î®Ï„Ï‰Î½ Î³Î¹Î± ROC-AUC (ÎºÎ¬Ï€Î¿Î¹Î± Î¼Î¿Î½Ï„Î­Î»Î± ÏŒÏ€Ï‰Ï‚ SVM-linear Î´ÎµÎ½ Î­Ï‡Î¿Ï…Î½ predict_proba)
    try:
        probs = model.predict_proba(X_test)[:, 1]
        has_probs = True
    except (AttributeError, NotImplementedError):
        has_probs = False
        print(f"âš ï¸ Warning: Model usually doesn't support probability output for this config.")

    inference_time = time.time() - start_time

    # 2. Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ Metrics
    acc = accuracy_score(y_test, predictions)
    f1 = f1_score(y_test, predictions)
    precision = precision_score(y_test, predictions)
    recall = recall_score(y_test, predictions)

    # ÎšÎ±Ï„Î±Î³ÏÎ±Ï†Î® Î½Î¿ÏÎ¼ÎµÏÏ‰Î½
    mlflow.log_metric(f"{prefix}_accuracy", acc)
    mlflow.log_metric(f"{prefix}_f1", f1)
    mlflow.log_metric(f"{prefix}_precision", precision)
    mlflow.log_metric(f"{prefix}_recall", recall)
    mlflow.log_metric(f"{prefix}_inference_time_sec", inference_time)

    if has_probs:
        auc = roc_auc_score(y_test, probs)
        mlflow.log_metric(f"{prefix}_roc_auc", auc)

    # 3. Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Confusion Matrix Plot
    cm = confusion_matrix(y_test, predictions)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.title(f'Confusion Matrix ({prefix})')
    plt.ylabel('Actual')
    plt.xlabel('Predicted')

    # Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· ÎµÎ¹ÎºÏŒÎ½Î±Ï‚ ÎºÎ±Î¹ upload ÏƒÏ„Î¿ MLflow
    cm_filename = f"confusion_matrix_{prefix}.png"
    plt.savefig(cm_filename)
    mlflow.log_artifact(cm_filename)
    plt.close()

    # 4. Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± ROC Curve Plot (Î‘Î½ Î­Ï‡Î¿Ï…Î¼Îµ Ï€Î¹Î¸Î±Î½ÏŒÏ„Î·Ï„ÎµÏ‚)
    if has_probs:
        fpr, tpr, _ = roc_curve(y_test, probs)
        plt.figure(figsize=(6, 5))
        plt.plot(fpr, tpr, label=f'AUC = {auc:.2f}')
        plt.plot([0, 1], [0, 1], 'k--')  # Î”Î¹Î±Î³ÏÎ½Î¹Î¿Ï‚
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title(f'ROC Curve ({prefix})')
        plt.legend()

        roc_filename = f"roc_curve_{prefix}.png"
        plt.savefig(roc_filename)
        mlflow.log_artifact(roc_filename)
        plt.close()

    print(f"ğŸ“Š Metrics logged for {prefix}: F1={f1:.4f}, Acc={acc:.4f}")
    return f1