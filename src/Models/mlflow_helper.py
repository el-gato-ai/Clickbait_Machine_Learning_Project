import mlflow
import matplotlib.pyplot as plt
import seaborn as sns
import time
import numpy as np
import os
from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score,
                             roc_auc_score, confusion_matrix, roc_curve,
                             average_precision_score, precision_recall_curve, log_loss, classification_report)


def setup_mlflow(experiment_name):
    """
    Î¡Ï…Î¸Î¼Î¯Î¶ÎµÎ¹ Ï„Î¿ MLflow Î½Î± Î±Ï€Î¿Î¸Î·ÎºÎµÏÎµÎ¹ ÏƒÏ„Î¿Î½ Ï†Î¬ÎºÎµÎ»Î¿ 'mlruns' ÏƒÏ„Î¿ root Ï„Î¿Ï… project.
    """
    current_file_path = os.path.abspath(__file__)
    # Î¥Ï€Î¿Î¸Î­Ï„Î¿Ï…Î¼Îµ Î´Î¿Î¼Î®: src/Models/mlflow_helper.py -> root ÎµÎ¯Î½Î±Î¹ 2 ÎµÏ€Î¯Ï€ÎµÎ´Î± Ï€Î¬Î½Ï‰
    # Î‘Î½ Ï„Î¿ script ÎµÎ¯Î½Î±Î¹ ÏƒÏ„Î¿ Models/Random Forest..., Î¸Î­Î»Î¿Ï…Î¼Îµ 3 ÎµÏ€Î¯Ï€ÎµÎ´Î± Ï€Î¯ÏƒÏ‰.
    # Î“Î¹Î± Î±ÏƒÏ†Î¬Î»ÎµÎ¹Î±, ÏˆÎ¬Ï‡Î½Î¿Ï…Î¼Îµ Ï„Î¿ Ï†Î¬ÎºÎµÎ»Î¿ 'data' Î® 'mlruns' Ï€ÏÎ¿Ï‚ Ï„Î± Ï€Î¯ÏƒÏ‰.

    project_root = os.path.dirname(os.path.dirname(os.path.dirname(current_file_path)))
    mlruns_path = os.path.join(project_root, "mlruns")

    mlflow.set_tracking_uri(f"file://{mlruns_path}")
    mlflow.set_experiment(experiment_name)
    print(f"ğŸš€ MLflow tracking URI set to: {mlruns_path}")
    print(f"ğŸš€ MLflow experiment set to: {experiment_name}")


def log_optuna_trial(trial, params, metrics, model, model_name_artifact):
    """
    ÎšÎ±Ï„Î±Î³ÏÎ¬Ï†ÎµÎ¹ Î­Î½Î± trial Ï„Î¿Ï… Optuna.
    """
    with mlflow.start_run(nested=True):
        mlflow.log_params(params)
        mlflow.log_param("trial_number", trial.number)

        if isinstance(metrics, dict):
            mlflow.log_metrics(metrics)
        else:
            mlflow.log_metric("score", metrics)

        try:
            mlflow.sklearn.log_model(model, model_name_artifact)
        except Exception as e:
            print(f"âš ï¸ Î”ÎµÎ½ Î®Ï„Î±Î½ Î´Ï…Î½Î±Ï„Î® Î· Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Ï„Î¿Ï… Î¼Î¿Î½Ï„Î­Î»Î¿Ï…: {e}")


def evaluate_and_log_metrics(model, X_test, y_test, prefix="test", training_time=None):
    """
    Î¥Ï€Î¿Î»Î¿Î³Î¯Î¶ÎµÎ¹ metrics, Ï†Ï„Î¹Î¬Ï‡Î½ÎµÎ¹ Î³ÏÎ±Ï†Î®Î¼Î±Ï„Î± ÎºÎ±Î¹ Ï„Î± ÏƒÏ„Î­Î»Î½ÎµÎ¹ ÏƒÏ„Î¿ MLflow.
    Î”Î­Ï‡ÎµÏ„Î±Î¹ Ï€ÏÎ¿Î±Î¹ÏÎµÏ„Î¹ÎºÎ¬ Ï„Î¿ training_time Î³Î¹Î± Î½Î± Ï„Î¿ ÎºÎ±Ï„Î±Î³ÏÎ¬ÏˆÎµÎ¹.
    """
    start_time = time.time()

    # 1. Î ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚
    predictions = model.predict(X_test)

    # Î ÏÎ¿ÏƒÏ€Î¬Î¸ÎµÎ¹Î± Î»Î®ÏˆÎ·Ï‚ Ï€Î¹Î¸Î±Î½Î¿Ï„Î®Ï„Ï‰Î½
    try:
        probs = model.predict_proba(X_test)[:, 1]
        has_probs = True
    except (AttributeError, NotImplementedError):
        has_probs = False
        print(f"âš ï¸ Warning: Model usually doesn't support probability output for this config.")

    inference_time = time.time() - start_time

    # 2. Î’Î±ÏƒÎ¹ÎºÎ¬ Metrics
    acc = accuracy_score(y_test, predictions)
    f1 = f1_score(y_test, predictions)
    precision = precision_score(y_test, predictions)
    recall = recall_score(y_test, predictions)

    metrics_to_log = {
        f"{prefix}_accuracy": acc,
        f"{prefix}_f1": f1,
        f"{prefix}_precision": precision,
        f"{prefix}_recall": recall,
        f"{prefix}_inference_time_sec": inference_time
    }

    if training_time is not None:
        metrics_to_log[f"{prefix}_training_time_sec"] = training_time

    # 3. Advanced Metrics (Log Loss & PR Curve)
    if has_probs:
        auc = roc_auc_score(y_test, probs)
        ll = log_loss(y_test, probs)
        avg_prec = average_precision_score(y_test, probs)

        metrics_to_log[f"{prefix}_roc_auc"] = auc
        metrics_to_log[f"{prefix}_log_loss"] = ll
        metrics_to_log[f"{prefix}_average_precision"] = avg_prec

    mlflow.log_metrics(metrics_to_log)

    # 4. Plots
    # Confusion Matrix
    cm = confusion_matrix(y_test, predictions)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.title(f'Confusion Matrix ({prefix})')
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.savefig(f"confusion_matrix_{prefix}.png")
    mlflow.log_artifact(f"confusion_matrix_{prefix}.png")
    plt.close()

    if has_probs:
        # ROC Curve
        fpr, tpr, _ = roc_curve(y_test, probs)
        plt.figure(figsize=(6, 5))
        plt.plot(fpr, tpr, label=f'AUC = {auc:.2f}')
        plt.plot([0, 1], [0, 1], 'k--')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title(f'ROC Curve ({prefix})')
        plt.legend()
        plt.savefig(f"roc_curve_{prefix}.png")
        mlflow.log_artifact(f"roc_curve_{prefix}.png")
        plt.close()

        # Precision-Recall Curve
        prec_curve, rec_curve, _ = precision_recall_curve(y_test, probs)
        plt.figure(figsize=(6, 5))
        plt.plot(rec_curve, prec_curve, label=f'AP = {avg_prec:.2f}')
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.title(f'Precision-Recall Curve ({prefix})')
        plt.legend()
        plt.savefig(f"pr_curve_{prefix}.png")
        mlflow.log_artifact(f"pr_curve_{prefix}.png")
        plt.close()

    # Feature Importance (Î³Î¹Î± Tree-based models)
    if hasattr(model, "feature_importances_"):
        importances = model.feature_importances_
        # Î Î±Î¯ÏÎ½Î¿Ï…Î¼Îµ Ï„Î± top 20
        indices = np.argsort(importances)[::-1][:20]

        plt.figure(figsize=(10, 6))
        plt.title(f"Top 20 Feature Importances ({prefix})")
        plt.bar(range(len(indices)), importances[indices], align="center")
        plt.xticks(range(len(indices)), [f"umap_{i}" for i in indices], rotation=45)
        plt.tight_layout()
        plt.savefig(f"feature_importance_{prefix}.png")
        mlflow.log_artifact(f"feature_importance_{prefix}.png")
        plt.close()

    # Classification Report (Text)
    report = classification_report(y_test, predictions)
    with open(f"classification_report_{prefix}.txt", "w") as f:
        f.write(report)
    mlflow.log_artifact(f"classification_report_{prefix}.txt")

    print(f"ğŸ“Š Metrics logged for {prefix}: F1={f1:.4f}, Acc={acc:.4f}")
    return f1