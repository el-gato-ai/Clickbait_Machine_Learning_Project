import mlflow
import matplotlib.pyplot as plt
import seaborn as sns
import time
from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score,
                             roc_auc_score, confusion_matrix, roc_curve)

import os  # <--- Î£Î¹Î³Î¿Ï…ÏÎ­ÏˆÎ¿Ï… ÏŒÏ„Î¹ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ Î±Ï…Ï„ÏŒ ÏƒÏ„Î·Î½ Î±ÏÏ‡Î® Ï„Î¿Ï… Î±ÏÏ‡ÎµÎ¯Î¿Ï…

def setup_mlflow(experiment_name):
    """
    ÎŸÏÎ¯Î¶ÎµÎ¹ Ï„Î¿ ÏŒÎ½Î¿Î¼Î± Ï„Î¿Ï… Ï€ÎµÎ¹ÏÎ¬Î¼Î±Ï„Î¿Ï‚ ÎºÎ±Î¹ Î±Î½Î±Î³ÎºÎ¬Î¶ÎµÎ¹ Ï„Î·Î½ Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· 
    ÏƒÏ„Î¿Î½ ÎºÎµÎ½Ï„ÏÎ¹ÎºÏŒ Ï†Î¬ÎºÎµÎ»Î¿ mlruns Ï„Î¿Ï… project (Project Root).
    """
    # Î’ÏÎ¯ÏƒÎºÎ¿Ï…Î¼Îµ Ï„Î¿ Î¼Î¿Î½Î¿Ï€Î¬Ï„Î¹ Ï„Î¿Ï… Î±ÏÏ‡ÎµÎ¯Î¿Ï… mlflow_helper.py
    current_file_path = os.path.abspath(__file__)
    
    # Î Î·Î³Î±Î¯Î½Î¿Ï…Î¼Îµ 3 Ï†Î±ÎºÎ­Î»Î¿Ï…Ï‚ Ï€Î¯ÏƒÏ‰ Î³Î¹Î± Î½Î± Î²ÏÎ¿ÏÎ¼Îµ Ï„Î¿ root Ï„Î¿Ï… project
    # (Î±Ï€ÏŒ src/Models/mlflow_helper.py -> src/Models -> src -> Project Root)
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(current_file_path)))
    
    # ÎŸÏÎ¯Î¶Î¿Ï…Î¼Îµ Ï„Î¿Î½ Ï†Î¬ÎºÎµÎ»Î¿ mlruns ÏƒÏ„Î¿ root
    mlruns_path = os.path.join(project_root, "mlruns")
    
    # Î¡Ï…Î¸Î¼Î¯Î¶Î¿Ï…Î¼Îµ Ï„Î¿ MLflow Î½Î± ÎºÎ¿Î¹Ï„Î¬ÎµÎ¹ Î Î‘ÎÎ¤Î‘ ÎµÎºÎµÎ¯
    mlflow.set_tracking_uri(f"file://{mlruns_path}")
    
    mlflow.set_experiment(experiment_name)
    print(f"ğŸš€ MLflow tracking URI set to: {mlruns_path}")
    print(f"ğŸš€ MLflow experiment set to: {experiment_name}")

def log_optuna_trial(trial, params, metrics, model, model_name_artifact):
    """
    ÎšÎ±Ï„Î±Î³ÏÎ¬Ï†ÎµÎ¹ Ï„Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± ÎµÎ½ÏŒÏ‚ trial Ï„Î¿Ï… Optuna ÏƒÏ„Î¿ MLflow.
    Î”Î·Î¼Î¹Î¿Ï…ÏÎ³ÎµÎ¯ Î­Î½Î± nested run Î³Î¹Î± ÎºÎ¬Î¸Îµ Î´Î¿ÎºÎ¹Î¼Î®.
    """
    with mlflow.start_run(nested=True):
        # 1. ÎšÎ±Ï„Î±Î³ÏÎ±Ï†Î® Ï„Ï‰Î½ Ï€Î±ÏÎ±Î¼Î­Ï„ÏÏ‰Î½ Ï€Î¿Ï… Î´Î¹Î¬Î»ÎµÎ¾Îµ Ï„Î¿ Optuna
        mlflow.log_params(params)
        mlflow.log_param("trial_number", trial.number)

        # 2. ÎšÎ±Ï„Î±Î³ÏÎ±Ï†Î® Ï„Ï‰Î½ Metrics (F1, Accuracy ÎºÎ»Ï€)
        # Î‘Î½ Ï„Î¿ metrics ÎµÎ¯Î½Î±Î¹ Î»ÎµÎ¾Î¹ÎºÏŒ (dictionary), Ï„Î± ÎºÎ±Ï„Î±Î³ÏÎ¬Ï†Î¿Ï…Î¼Îµ ÏŒÎ»Î±
        if isinstance(metrics, dict):
            mlflow.log_metrics(metrics)
        else:
            # Î‘Î½ Î¼Î±Ï‚ Î®ÏÎ¸Îµ ÏƒÎºÎ­Ï„Î¿ Î½Î¿ÏÎ¼ÎµÏÎ¿ (Ï€.Ï‡. f1 score), Ï„Î¿ ÎºÎ±Ï„Î±Î³ÏÎ¬Ï†Î¿Ï…Î¼Îµ Ï‰Ï‚ score
            mlflow.log_metric("score", metrics)

        # 3. ÎšÎ±Ï„Î±Î³ÏÎ±Ï†Î® Ï„Î¿Ï… ÎœÎ¿Î½Ï„Î­Î»Î¿Ï…
        try:
            mlflow.sklearn.log_model(model, model_name_artifact)
        except Exception as e:
            print(f"âš ï¸ Î”ÎµÎ½ Î®Ï„Î±Î½ Î´Ï…Î½Î±Ï„Î® Î· Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Ï„Î¿Ï… Î¼Î¿Î½Ï„Î­Î»Î¿Ï…: {e}")
            
def evaluate_and_log_metrics(model, X_test, y_test, prefix="test"):
    """
    Î¥Ï€Î¿Î»Î¿Î³Î¯Î¶ÎµÎ¹ metrics, Ï†Ï„Î¹Î¬Ï‡Î½ÎµÎ¹ Î³ÏÎ±Ï†Î®Î¼Î±Ï„Î± ÎºÎ±Î¹ Ï„Î± ÏƒÏ„Î­Î»Î½ÎµÎ¹ ÏƒÏ„Î¿ MLflow.
    prefix: 'val' Î³Î¹Î± validation set, 'test' Î³Î¹Î± test set.
    """
    start_time = time.time()

    # 1. Î ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚
    predictions = model.predict(X_test)

    # Î ÏÎ¿ÏƒÏ€Î¬Î¸ÎµÎ¹Î± Î»Î®ÏˆÎ·Ï‚ Ï€Î¹Î¸Î±Î½Î¿Ï„Î®Ï„Ï‰Î½ Î³Î¹Î± ROC-AUC (ÎºÎ¬Ï€Î¿Î¹Î± Î¼Î¿Î½Ï„Î­Î»Î± ÏŒÏ€Ï‰Ï‚ SVM-linear Î´ÎµÎ½ Î­Ï‡Î¿Ï…Î½ predict_proba)
    try:
        probs = model.predict_proba(X_test)[:, 1]
        has_probs = True
    except (AttributeError, NotImplementedError):
        has_probs = False
        print(f"âš ï¸ Warning: Model usually doesn't support probability output for this config.")

    inference_time = time.time() - start_time

    # 2. Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ Metrics
    acc = accuracy_score(y_test, predictions)
    f1 = f1_score(y_test, predictions)
    precision = precision_score(y_test, predictions)
    recall = recall_score(y_test, predictions)

    # ÎšÎ±Ï„Î±Î³ÏÎ±Ï†Î® Î½Î¿ÏÎ¼ÎµÏÏ‰Î½
    mlflow.log_metric(f"{prefix}_accuracy", acc)
    mlflow.log_metric(f"{prefix}_f1", f1)
    mlflow.log_metric(f"{prefix}_precision", precision)
    mlflow.log_metric(f"{prefix}_recall", recall)
    mlflow.log_metric(f"{prefix}_inference_time_sec", inference_time)

    if has_probs:
        auc = roc_auc_score(y_test, probs)
        mlflow.log_metric(f"{prefix}_roc_auc", auc)

    # 3. Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Confusion Matrix Plot
    cm = confusion_matrix(y_test, predictions)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.title(f'Confusion Matrix ({prefix})')
    plt.ylabel('Actual')
    plt.xlabel('Predicted')

    # Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· ÎµÎ¹ÎºÏŒÎ½Î±Ï‚ ÎºÎ±Î¹ upload ÏƒÏ„Î¿ MLflow
    cm_filename = f"confusion_matrix_{prefix}.png"
    plt.savefig(cm_filename)
    mlflow.log_artifact(cm_filename)
    plt.close()

    # 4. Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± ROC Curve Plot (Î‘Î½ Î­Ï‡Î¿Ï…Î¼Îµ Ï€Î¹Î¸Î±Î½ÏŒÏ„Î·Ï„ÎµÏ‚)
    if has_probs:
        fpr, tpr, _ = roc_curve(y_test, probs)
        plt.figure(figsize=(6, 5))
        plt.plot(fpr, tpr, label=f'AUC = {auc:.2f}')
        plt.plot([0, 1], [0, 1], 'k--')  # Î”Î¹Î±Î³ÏÎ½Î¹Î¿Ï‚
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title(f'ROC Curve ({prefix})')
        plt.legend()

        roc_filename = f"roc_curve_{prefix}.png"
        plt.savefig(roc_filename)
        mlflow.log_artifact(roc_filename)
        plt.close()

    print(f"ğŸ“Š Metrics logged for {prefix}: F1={f1:.4f}, Acc={acc:.4f}")
    return f1