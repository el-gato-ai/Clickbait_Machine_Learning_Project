{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Clickbait Detection: SVM (Scaled) & UMAP Embeddings\n",
    "\n",
    "## Î•Î¹ÏƒÎ±Î³Ï‰Î³Î®\n",
    "Î£Ï„Î¿ Ï€Î±ÏÏŒÎ½ notebook Î±Î½Î±Ï€Ï„ÏÏƒÏƒÎ¿Ï…Î¼Îµ Î­Î½Î± Î¼Î¿Î½Ï„Î­Î»Î¿ **Support Vector Machine (SVM)** Î¼Îµ Ï€Ï…ÏÎ®Î½Î± RBF.\n",
    "\n",
    "### Î“Î¹Î±Ï„Î¯ Scaling;\n",
    "Î¤Î± Î¼Î¿Î½Ï„Î­Î»Î± SVM Ï€Î¿Ï… Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ½ Ï€Ï…ÏÎ®Î½ÎµÏ‚ (kernels) ÏŒÏ€Ï‰Ï‚ Î¿ RBF Î²Î±ÏƒÎ¯Î¶Î¿Î½Ï„Î±Î¹ ÏƒÏ„Î¿Î½ Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒ Î•Ï…ÎºÎ»ÎµÎ¯Î´ÎµÎ¹Ï‰Î½ Î±Ï€Î¿ÏƒÏ„Î¬ÏƒÎµÏ‰Î½ Î¼ÎµÏ„Î±Î¾Ï Ï„Ï‰Î½ ÏƒÎ·Î¼ÎµÎ¯Ï‰Î½.\n",
    "Î•Î¬Î½ Ï„Î± Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ Î´ÎµÎ½ ÎµÎ¯Î½Î±Î¹ ÏƒÏ„Î·Î½ Î¯Î´Î¹Î± ÎºÎ»Î¯Î¼Î±ÎºÎ±, Î¿Î¹ Î´Î¹Î±ÏƒÏ„Î¬ÏƒÎµÎ¹Ï‚ Î¼Îµ Î¼ÎµÎ³Î¬Î»ÎµÏ‚ Ï„Î¹Î¼Î­Ï‚/Î´Î¹Î±ÎºÏ…Î¼Î¬Î½ÏƒÎµÎ¹Ï‚ Î¸Î± ÎºÏ…ÏÎ¹Î±ÏÏ‡Î®ÏƒÎ¿Ï…Î½ ÏƒÏ„Î¿Î½ Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒ Ï„Î·Ï‚ Î±Ï€ÏŒÏƒÏ„Î±ÏƒÎ·Ï‚, Î¿Î´Î·Î³ÏÎ½Ï„Î±Ï‚ ÏƒÎµ ÎºÎ±ÎºÎ® Î±Ï€ÏŒÎ´Î¿ÏƒÎ·.\n",
    "Î“Î¹' Î±Ï…Ï„ÏŒ, Î· ÎµÏ†Î±ÏÎ¼Î¿Î³Î® **`StandardScaler`** ÎµÎ¯Î½Î±Î¹ Ï…Ï€Î¿Ï‡ÏÎµÏ‰Ï„Î¹ÎºÎ®.\n",
    "\n",
    "## Î£Ï„ÏŒÏ‡Î¿Î¹\n",
    "1. **Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½**: UMAP-500 embeddings.\n",
    "2. **Î’ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¯Î·ÏƒÎ· (Optuna)**: Î•ÏÏÎµÏƒÎ· Ï„Ï‰Î½ Î²Î­Î»Ï„Î¹ÏƒÏ„Ï‰Î½ `C`, `tol` ÎºÎ±Î¹ `class_weight`.\n",
    "3. **Champion Model**: Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· Î¼Îµ `max_iter=15000` Î³Î¹Î± Î´Î¹Î±ÏƒÏ†Î¬Î»Î¹ÏƒÎ· ÏƒÏÎ³ÎºÎ»Î¹ÏƒÎ·Ï‚.\n",
    "4. **Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ·**: Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Ï„Î¿Ï… Scaler Î¼Î±Î¶Î¯ Î¼Îµ Ï„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿."
   ],
   "id": "4c5301052c6e3df6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import mlflow\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Î‘Ï€ÏŒÎºÏÏ…ÏˆÎ· FutureWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn\")\n",
    "\n",
    "# Î¡ÏÎ¸Î¼Î¹ÏƒÎ· Paths\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "import mlflow_helper\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '../../..'))\n",
    "DATA_FOLDER = os.path.join(project_root, 'data', 'clean', 'umap')\n",
    "\n",
    "print(f\"Project Root: {project_root}\")\n",
    "print(f\"Data Folder: {DATA_FOLDER}\")"
   ],
   "id": "1083aa46fc1e7271"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½\n",
    "\n",
    "Î¦Î¿ÏÏ„ÏÎ½Î¿Ï…Î¼Îµ Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î±. Î¤Î¿ Scaling Î¸Î± ÎµÏ†Î±ÏÎ¼Î¿ÏƒÏ„ÎµÎ¯ ÏƒÏ„Î¿ ÎµÏ€ÏŒÎ¼ÎµÎ½Î¿ Î²Î®Î¼Î±."
   ],
   "id": "792dbdf776b24f7d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_split_data(data_path):\n",
    "    files = {\n",
    "        \"Train\": \"train_umap_500.parquet\",\n",
    "        \"Valid\": \"valid_umap_500.parquet\",\n",
    "        \"Test\":  \"test_umap_500.parquet\"\n",
    "    }\n",
    "\n",
    "    loaded_data = {}\n",
    "    possible_label_cols = ['labels', 'label', 'target', 'class', 'is_clickbait']\n",
    "\n",
    "    print(f\"â³ ÎˆÎ½Î±ÏÎ¾Î· Ï†ÏŒÏÏ„Ï‰ÏƒÎ·Ï‚ Î±Ï€ÏŒ: {data_path}\")\n",
    "\n",
    "    for name, filename in files.items():\n",
    "        file_path = os.path.join(data_path, filename)\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"Î¤Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ {filename} Î´ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ.\")\n",
    "\n",
    "        try:\n",
    "            df = pd.read_parquet(file_path, engine='fastparquet')\n",
    "        except:\n",
    "            df = pd.read_parquet(file_path, engine='pyarrow')\n",
    "\n",
    "        # 1. Î•Î½Ï„Î¿Ï€Î¹ÏƒÎ¼ÏŒÏ‚ Features\n",
    "        feature_cols = sorted([c for c in df.columns if c.startswith(\"umap_\")])\n",
    "\n",
    "        # 2. Î•Î½Ï„Î¿Ï€Î¹ÏƒÎ¼ÏŒÏ‚ Label\n",
    "        label_col = next((c for c in possible_label_cols if c in df.columns), None)\n",
    "        if label_col is None:\n",
    "             raise ValueError(f\"Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ label ÏƒÏ„Î®Î»Î· ÏƒÏ„Î¿ {filename}\")\n",
    "\n",
    "        # Î”Î¹Î±Ï‡Ï‰ÏÎ¹ÏƒÎ¼ÏŒÏ‚ X, y\n",
    "        y = df[label_col].values.astype(int)\n",
    "        X = df[feature_cols].values.astype(np.float32)\n",
    "\n",
    "        loaded_data[name] = (X, y)\n",
    "        print(f\"   âœ… [{name}] Loaded: X={X.shape}, y={y.shape}\")\n",
    "\n",
    "    return loaded_data[\"Train\"], loaded_data[\"Valid\"], loaded_data[\"Test\"]\n",
    "\n",
    "# Î•ÎºÏ„Î­Î»ÎµÏƒÎ· Î¦ÏŒÏÏ„Ï‰ÏƒÎ·Ï‚\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test) = load_split_data(DATA_FOLDER)"
   ],
   "id": "e99ac1a61a79d97d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Î’ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¯Î·ÏƒÎ· Î¥Ï€ÎµÏÏ€Î±ÏÎ±Î¼Î­Ï„ÏÏ‰Î½ (Tuning)\n",
    "\n",
    "ÎŸÏÎ¯Î¶Î¿Ï…Î¼Îµ Ï„Î· ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· `objective`.\n",
    "* **Kernel**: Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ ÏƒÏ„Î±Î¸ÎµÏÎ¬ `rbf` Î¼Îµ `gamma='auto'`.\n",
    "* **Scaling**: Î•Ï†Î±ÏÎ¼ÏŒÎ¶Î¿Ï…Î¼Îµ `StandardScaler` ÎµÎ½Ï„ÏŒÏ‚ Ï„Î¿Ï… trial (fit ÏƒÏ„Î¿ train, transform ÏƒÏ„Î¿ valid).\n",
    "* **Max Iterations**: ÎŸÏÎ¯Î¶Î¿Ï…Î¼Îµ `max_iter=15000` Î³Î¹Î±Ï„Î¯ Ï„Î¿ SVM ÎµÎ¯Î½Î±Î¹ Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÏ„Î¹ÎºÎ¬ Î²Î±ÏÏ ÎºÎ±Î¹ Î¸Î­Î»Î¿Ï…Î¼Îµ Î½Î± Ï„Î¿Ï… Î´ÏÏƒÎ¿Ï…Î¼Îµ Ï€ÎµÏÎ¹Î¸ÏÏÎ¹Î¿ ÏƒÏÎ³ÎºÎ»Î¹ÏƒÎ·Ï‚."
   ],
   "id": "5f13298244d6a5eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def objective(trial, X_tr, y_tr, X_v, y_v):\n",
    "    # --- 1. Hyperparameters Space ---\n",
    "    C = trial.suggest_float(\"C\", 1e-1, 1e2, log=True)\n",
    "    tol = trial.suggest_float(\"tol\", 1e-5, 5e-4, log=True)\n",
    "    class_weight = trial.suggest_categorical(\"class_weight\", [None, \"balanced\"])\n",
    "\n",
    "    # Î£Ï„Î±Î¸ÎµÏÎ­Ï‚ Ï€Î±ÏÎ¬Î¼ÎµÏ„ÏÎ¿Î¹ Î³Î¹Î± SVM\n",
    "    kernel = \"rbf\"\n",
    "    gamma = \"auto\"\n",
    "    max_iter = 15000\n",
    "\n",
    "    # --- 2. Scaling (Critical for SVM) ---\n",
    "    scaler = StandardScaler()\n",
    "    X_tr_sc = scaler.fit_transform(X_tr)\n",
    "    X_v_sc = scaler.transform(X_v)\n",
    "\n",
    "    # --- 3. Model Setup ---\n",
    "    params = dict(\n",
    "        kernel=kernel,\n",
    "        gamma=gamma,\n",
    "        C=C,\n",
    "        tol=tol,\n",
    "        class_weight=class_weight,\n",
    "        max_iter=max_iter,\n",
    "        probability=False, # False Î³Î¹Î± Ï„Î±Ï‡ÏÏ„Î·Ï„Î± ÏƒÏ„Î¿ tuning\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model = SVC(**params)\n",
    "\n",
    "    # --- 4. Training & Eval ---\n",
    "    t0 = time.time()\n",
    "    model.fit(X_tr_sc, y_tr)\n",
    "    train_time = time.time() - t0\n",
    "\n",
    "    preds = model.predict(X_v_sc)\n",
    "    f1 = f1_score(y_v, preds)\n",
    "    acc = accuracy_score(y_v, preds)\n",
    "\n",
    "    metrics = {\"val_f1\": f1, \"val_accuracy\": acc, \"training_time_sec\": train_time}\n",
    "\n",
    "    # Log scaling info\n",
    "    params_to_log = params.copy()\n",
    "    params_to_log[\"scaler\"] = \"StandardScaler\"\n",
    "\n",
    "    mlflow_helper.log_optuna_trial(trial, params_to_log, metrics, model, run_name_prefix=\"SVM_Trial\")\n",
    "\n",
    "    return f1"
   ],
   "id": "574967737d011405"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Î•ÎºÏ„Î­Î»ÎµÏƒÎ· Î ÎµÎ¹ÏÎ¬Î¼Î±Ï„Î¿Ï‚ & Champion Model\n",
    "\n",
    "1. **Tuning:** 10 trials (Î»ÏŒÎ³Ï‰ Ï„Î¿Ï… Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÏ„Î¹ÎºÎ¿Ï ÎºÏŒÏƒÏ„Î¿Ï…Ï‚ Ï„Î¿Ï… SVM).\n",
    "2. **Champion Model:**\n",
    "   - Î•Ï€Î±Î½ÎµÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· ÏƒÏ„Î¿ Full Dataset (Train + Valid) Î¼Îµ **Scaling**.\n",
    "   - Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Ï„Î¿Ï… `scaler.pkl` Ï‰Ï‚ artifact.\n",
    "   - Î‘Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ· ÏƒÏ„Î¿ Test set (Î±Ï†Î¿Ï Î³Î¯Î½ÎµÎ¹ transform Î¼Îµ Ï„Î¿Î½ Î¯Î´Î¹Î¿ scaler)."
   ],
   "id": "2084dde55e732740"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "EXPERIMENT_NAME = \"Clickbait_SVM_UMAP500_Optuna_Scaled\"\n",
    "mlflow_helper.setup_mlflow(EXPERIMENT_NAME)\n",
    "\n",
    "print(f\"\\nğŸš€ ÎˆÎ½Î±ÏÎ¾Î· Î ÎµÎ¹ÏÎ¬Î¼Î±Ï„Î¿Ï‚: {EXPERIMENT_NAME}\")\n",
    "\n",
    "# --- Î¦Î‘Î£Î— 1: Tuning ---\n",
    "print(\"\\nğŸ” [Î¦Î‘Î£Î— 1] Hyperparameter Tuning...\")\n",
    "with mlflow.start_run(run_name=\"SVM_Hyperparameter_Tuning\"):\n",
    "    mlflow.log_param(\"dataset\", \"UMAP_500\")\n",
    "    mlflow.log_param(\"scaler\", \"StandardScaler\")\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    # 10 Trials Î¼ÏŒÎ½Î¿, Î³Î¹Î±Ï„Î¯ Ï„Î¿ SVM ÎµÎ¯Î½Î±Î¹ Î±ÏÎ³ÏŒ\n",
    "    study.optimize(\n",
    "        lambda t: objective(t, X_train, y_train, X_val, y_val),\n",
    "        n_trials=10\n",
    "    )\n",
    "\n",
    "    print(\"ğŸ† Best F1:\", study.best_value)\n",
    "    print(\"ğŸ† Best Params:\", study.best_params)\n",
    "\n",
    "\n",
    "# --- Î¦Î‘Î£Î— 2: Champion Model ---\n",
    "print(\"\\nğŸ‘‘ [Î¦Î‘Î£Î— 2] Champion Model Training...\")\n",
    "with mlflow.start_run(run_name=\"SVM_Champion_Model\") as final_run:\n",
    "    best = study.best_params\n",
    "\n",
    "    # 1. Î ÏÎ¿ÎµÏ„Î¿Î¹Î¼Î±ÏƒÎ¯Î± Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ & Scaling\n",
    "    X_full = np.vstack([X_train, X_val])\n",
    "    y_full = np.concatenate([y_train, y_val])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_full_sc = scaler.fit_transform(X_full)\n",
    "    X_test_sc = scaler.transform(X_test)\n",
    "\n",
    "    # 2. Setup Model\n",
    "    champion_params = dict(\n",
    "        kernel=\"rbf\",\n",
    "        gamma=\"auto\",\n",
    "        C=best[\"C\"],\n",
    "        tol=best[\"tol\"],\n",
    "        class_weight=best[\"class_weight\"],\n",
    "        max_iter=15000,\n",
    "        probability=False,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    final_model = SVC(**champion_params)\n",
    "\n",
    "    # 3. Training\n",
    "    t0 = time.time()\n",
    "    final_model.fit(X_full_sc, y_full)\n",
    "    final_train_time = time.time() - t0\n",
    "\n",
    "    # 4. Logging & Artifacts\n",
    "    mlflow.log_params(champion_params)\n",
    "    mlflow.log_param(\"scaler\", \"StandardScaler\")\n",
    "\n",
    "    with open(\"scaler.pkl\", \"wb\") as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    mlflow.log_artifact(\"scaler.pkl\")\n",
    "\n",
    "    mlflow.sklearn.log_model(final_model, artifact_path=\"champion_model\")\n",
    "\n",
    "    # 5. Evaluation\n",
    "    print(\"ğŸ“ˆ Î‘Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ· ÏƒÏ„Î¿ Test Set...\")\n",
    "    mlflow_helper.evaluate_and_log_metrics(\n",
    "        final_model,\n",
    "        X_test_sc, # Scaled Data!\n",
    "        y_test,\n",
    "        prefix=\"test\",\n",
    "        training_time=final_train_time\n",
    "    )\n",
    "\n",
    "    print(f\"\\nâœ… ÎŸÎ»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ. Run ID: {final_run.info.run_id}\")"
   ],
   "id": "4e57799c5dd9ebf8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
